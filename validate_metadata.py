#!/usr/bin/env python3
"""
Metadata Validation Script for ComfyUI Workflow Test Outputs

This script validates that images generated by ComfyUI workflows contain the expected
metadata based on the workflow configuration. It reads metadata from PNG, JPEG, and WebP
images and compares it against expected values derived from the workflow JSON files.

Usage:
    python validate_metadata.py --output-folder "path/to/output/Test" [options]

Requirements:
    - Pillow (PIL)
    - piexif (for EXIF reading)
"""

import argparse
import json
import re
import sys
from pathlib import Path
from typing import Any

try:
    from PIL import Image
    from PIL.ExifTags import TAGS
except ImportError:
    print("Error: Pillow is required. Install with: pip install Pillow")
    sys.exit(1)

try:
    import piexif
    PIEXIF_AVAILABLE = True
except ImportError:
    PIEXIF_AVAILABLE = False


class MetadataReader:
    """Reads metadata from various image formats."""

    @staticmethod
    def read_png_metadata(image_path: Path) -> dict[str, str]:
        """Read metadata from PNG file."""
        metadata = {}
        try:
            img = Image.open(image_path)

            # Try to get PNG info
            if hasattr(img, 'info') and img.info:
                # Look for parameters in PNG metadata
                if 'parameters' in img.info:
                    metadata['parameters'] = img.info['parameters']

                # Copy all text chunks
                for key, value in img.info.items():
                    if isinstance(value, str):
                        metadata[key] = value

            # Alternative: read from binary if structured metadata not found
            if not metadata:
                with open(image_path, 'rb') as f:
                    binary_content = f.read()
                    start_index = binary_content.find(b'parameters')
                    if start_index != -1:
                        # Try to extract parameters
                        near_end_index = binary_content.find(b'Hashes: ', start_index)
                        if near_end_index != -1:
                            end_index = binary_content.find(b'tEXt', near_end_index)
                            if end_index != -1:
                                # PNG tEXt chunk has CRC (4 bytes) + length (4 bytes) + type (4 bytes)
                                # We subtract 9 to account for partial chunk structure
                                PNG_TEXT_CHUNK_SUFFIX_LENGTH = 9
                                start_pos = start_index + len(b'parameters') + 1
                                end_pos = end_index - PNG_TEXT_CHUNK_SUFFIX_LENGTH
                                extracted = binary_content[start_pos:end_pos]
                                try:
                                    metadata['parameters'] = extracted.decode('utf-8').strip()
                                except UnicodeDecodeError:
                                    # Decoding failed; skip and continue with other metadata extraction methods
                                    pass
        except Exception as e:
            print(f"  Warning: Error reading PNG metadata from {image_path.name}: {e}")

        return metadata

    @staticmethod
    def decode_user_comment(user_comment: bytes) -> str:
        """Decode EXIF UserComment field."""
        try:
            comment_bytes = user_comment
            # Skip first 8 bytes (character code) if present
            if len(comment_bytes) > 8 and comment_bytes[:8] == b'UNICODE\x00':
                comment_bytes = comment_bytes[8:]
            elif len(comment_bytes) > 4:
                # Try to skip encoding marker
                comment_bytes = comment_bytes[4:]

            # Try UTF-16 BE first
            try:
                return comment_bytes.decode('utf-16be', 'backslashreplace')
            except UnicodeDecodeError:
                # UTF-16 BE decoding failed; try other encodings
                pass

            # Try UTF-8
            try:
                return comment_bytes.decode('utf-8', 'backslashreplace')
            except UnicodeDecodeError:
                # UTF-8 decoding failed; fall back to latin-1
                pass

            # Fallback to latin-1
            return comment_bytes.decode('latin-1', 'replace')
        except Exception:
            return str(user_comment)

    @staticmethod
    def read_jpeg_metadata(image_path: Path) -> dict[str, str]:
        """Read metadata from JPEG file."""
        metadata = {}
        try:
            img = Image.open(image_path)

            # Try using piexif if available
            if PIEXIF_AVAILABLE:
                try:
                    exif_dict = piexif.load(str(image_path))
                    if piexif.ExifIFD.UserComment in exif_dict.get("Exif", {}):
                        user_comment = exif_dict["Exif"][piexif.ExifIFD.UserComment]
                        metadata['parameters'] = MetadataReader.decode_user_comment(user_comment)
                except Exception as e:
                    print(f"  Warning: piexif failed to read EXIF from {image_path.name}: {e}")

            # Fallback to PIL's EXIF reading
            if not metadata and hasattr(img, '_getexif') and img._getexif():
                exif_data = img._getexif()
                for tag, value in exif_data.items():
                    tag_name = TAGS.get(tag, tag)
                    if tag_name == 'UserComment' and isinstance(value, bytes):
                        decoded = MetadataReader.decode_user_comment(value)
                        metadata['parameters'] = decoded
                        break

            # Check for JPEG comment marker (fallback mode)
            if not metadata:
                with open(image_path, 'rb') as f:
                    content = f.read()
                    # Look for COM marker
                    # JPEG COM (comment) marker is represented by the byte sequence 0xFF 0xFE.
                    # This check detects if the image contains a comment marker, used for fallback metadata.
                    if b'\xff\xfe' in content:
                        metadata['_fallback_mode'] = 'com-marker'
        except Exception as e:
            print(f"  Warning: Error reading JPEG metadata from {image_path.name}: {e}")

        return metadata

    @staticmethod
    def read_webp_metadata(image_path: Path) -> dict[str, str]:
        """Read metadata from WebP file."""
        metadata = {}
        try:
            img = Image.open(image_path)

            # WebP can have EXIF data
            if PIEXIF_AVAILABLE and 'exif' in img.info:
                try:
                    exif_dict = piexif.load(img.info['exif'])
                    if piexif.ExifIFD.UserComment in exif_dict.get("Exif", {}):
                        user_comment = exif_dict["Exif"][piexif.ExifIFD.UserComment]
                        metadata['parameters'] = MetadataReader.decode_user_comment(user_comment)
                except Exception as exif_error:
                    print(f"  Warning: Error reading EXIF from WebP {image_path.name}: {exif_error}")

            # Check other WebP metadata
            if hasattr(img, 'info'):
                for key, value in img.info.items():
                    if isinstance(value, str):
                        metadata[key] = value
        except Exception as e:
            print(f"  Warning: Error reading WebP metadata from {image_path.name}: {e}")

        return metadata

    @staticmethod
    def read_metadata(image_path: Path) -> dict[str, str]:
        """Read metadata from any supported image format."""
        ext = image_path.suffix.lower()

        if ext == '.png':
            return MetadataReader.read_png_metadata(image_path)
        elif ext in ['.jpg', '.jpeg']:
            return MetadataReader.read_jpeg_metadata(image_path)
        elif ext == '.webp':
            return MetadataReader.read_webp_metadata(image_path)
        else:
            print(f"  Warning: Unsupported image format: {ext}")
            return {}


class WorkflowAnalyzer:
    """Analyzes workflow JSON files to extract expected metadata."""

    @staticmethod
    def find_save_node(workflow: dict) -> tuple[str | None, dict | None]:
        """Find the Save Image node in the workflow."""
        for node_id, node_data in workflow.items():
            if node_data.get('class_type') == 'SaveImageWithMetaDataUniversal':
                return node_id, node_data
        return None, None

    @staticmethod
    def find_sampler_nodes(workflow: dict) -> list[tuple[str, dict]]:
        """Find all sampler-like nodes in the workflow."""
        sampler_types = [
            'KSampler',
            'KSamplerAdvanced',
            'SamplerCustom',
            'SamplerCustomAdvanced',
        ]
        samplers = []
        for node_id, node_data in workflow.items():
            if node_data.get('class_type') in sampler_types:
                samplers.append((node_id, node_data))
        return samplers

    @staticmethod
    def extract_expected_metadata(workflow: dict, workflow_name: str) -> dict[str, Any]:
        """Extract expected metadata fields from workflow.

        Returns a dictionary with:
        - workflow_name: Name of the workflow file
        - has_save_node: Whether a SaveImageWithMetaDataUniversal node exists
        - file_format: Expected output format (png/jpeg/webp)
        - filename_prefix: Output filename prefix template
        - save_workflow_json: Whether workflow JSON should be saved
        - include_lora_summary: Whether LoRA summary line is included
        - max_jpeg_exif_kb: Maximum JPEG EXIF size before fallback
        - sampler_info: List of sampler node configurations (steps, cfg, seed, etc.)
        """
        expected = {
            'workflow_name': workflow_name,
            'has_save_node': False,
            'file_format': None,
            'sampler_info': [],
        }

        # Find save node
        save_node_id, save_node = WorkflowAnalyzer.find_save_node(workflow)
        if save_node:
            expected['has_save_node'] = True
            inputs = save_node.get('inputs', {})
            expected['file_format'] = inputs.get('file_format', 'png')
            expected['filename_prefix'] = inputs.get('filename_prefix', '')
            expected['save_workflow_json'] = inputs.get('save_workflow_json', False)
            expected['include_lora_summary'] = inputs.get('include_lora_summary', False)
            expected['max_jpeg_exif_kb'] = inputs.get('max_jpeg_exif_kb', 60)

        # Find sampler nodes
        samplers = WorkflowAnalyzer.find_sampler_nodes(workflow)
        for sampler_id, sampler_node in samplers:
            sampler_inputs = sampler_node.get('inputs', {})
            sampler_info = {
                'node_id': sampler_id,
                'class_type': sampler_node.get('class_type'),
                'steps': sampler_inputs.get('steps'),
                'cfg': sampler_inputs.get('cfg'),
                'sampler_name': sampler_inputs.get('sampler_name'),
                'scheduler': sampler_inputs.get('scheduler'),
                'denoise': sampler_inputs.get('denoise'),
                'seed': None,
            }

            # Try to find seed (might be in noise input or direct)
            if 'seed' in sampler_inputs:
                sampler_info['seed'] = sampler_inputs['seed']

            expected['sampler_info'].append(sampler_info)

        return expected


class MetadataValidator:
    """Validates image metadata against expected values."""

    def __init__(self, workflow_dir: Path, output_dir: Path):
        self.workflow_dir = workflow_dir
        self.output_dir = output_dir
        self.results = []

    def parse_parameters_string(self, params_str: str) -> dict[str, str]:
        """Parse the parameters string into a dictionary of fields.

        Only treats lines as key-value pairs if the key matches a known metadata key pattern.
        This prevents incorrectly splitting prompt text containing colons (e.g., "at 3:00 PM" or URLs).
        """
        if not params_str:
            return {}

        # Define known metadata keys that should be treated as field names
        # These are the standard metadata fields from the Save Image node
        known_keys = {
            "Steps", "Sampler", "CFG scale", "Seed", "Size", "Model", "Model hash",
            "VAE", "VAE hash", "Clip skip", "Denoise", "Shift", "Max shift", "Base shift",
            "Guidance", "Scheduler", "Hashes", "Metadata generator version", "Batch index",
            "Batch size", "Metadata Fallback", "LoRAs"
        }
        # Also match keys like "Lora_1", "Lora_2", "Embedding_1", etc.
        known_key_patterns = [r"^Lora_\d+$", r"^Lora_\d+ hash$", r"^Embedding_\d+$", r"^Embedding_\d+ hash$"]

        fields = {}
        lines = params_str.strip().split('\n')
        current_key = None
        current_value = []

        for line in lines:
            # Check if line starts with a known key followed by a colon
            match = re.match(r'^([A-Za-z0-9 _\-]+):\s*(.*)', line)
            if match:
                potential_key = match.group(1).strip()
                value = match.group(2)

                # Check if this is a known metadata key
                is_known_key = potential_key in known_keys
                if not is_known_key:
                    # Check against patterns
                    for pattern in known_key_patterns:
                        if re.match(pattern, potential_key):
                            is_known_key = True
                            break

                if is_known_key:
                    # Save previous key-value if exists
                    if current_key:
                        fields[current_key] = '\n'.join(current_value).strip()

                    current_key = potential_key
                    current_value = [value] if value else []
                    continue

            # If we get here, this line is either a continuation or not a field
            if current_key:
                # Continuation of previous value
                current_value.append(line.strip())

        # Save last key-value
        if current_key:
            fields[current_key] = '\n'.join(current_value).strip()

        return fields

    def validate_image(self, image_path: Path, workflow_name: str, expected: dict) -> dict:
        """Validate a single image's metadata."""
        result = {
            'image_path': str(image_path),
            'workflow_name': workflow_name,
            'passed': False,
            'errors': [],
            'warnings': [],
            'metadata_found': False,
            'fields': {},
        }

        # Read metadata
        metadata = MetadataReader.read_metadata(image_path)

        if not metadata:
            result['errors'].append("No metadata found in image")
            return result

        result['metadata_found'] = True

        # Get parameters string
        params_str = metadata.get('parameters', '')
        if not params_str:
            result['errors'].append("No 'parameters' field found in metadata")
            return result

        # Parse parameters
        fields = self.parse_parameters_string(params_str)
        result['fields'] = fields

        # Check for required fields based on workflow
        required_fields = []
        if expected.get('sampler_info'):
            # If workflow has samplers, we expect sampler metadata
            sampler = expected['sampler_info'][0]  # Use first sampler
            if sampler.get('steps'):
                required_fields.append('Steps')
            if sampler.get('sampler_name'):
                required_fields.append('Sampler')
            if sampler.get('cfg'):
                required_fields.append('CFG scale')
            if sampler.get('seed') is not None:
                required_fields.append('Seed')

        # Validate required fields
        for field in required_fields:
            if field not in fields:
                result['errors'].append(f"Required field '{field}' not found in metadata")

        # Check for fallback indicator
        if 'Metadata Fallback:' in params_str:
            fallback_match = re.search(r'Metadata Fallback:\s*(\S+)', params_str)
            if fallback_match:
                fallback_stage = fallback_match.group(1)
                result['warnings'].append(f"Metadata fallback occurred: {fallback_stage}")
                result['fallback_stage'] = fallback_stage

        # Validate file format matches expectation
        expected_format = expected.get('file_format', 'png')
        actual_format = image_path.suffix.lower().lstrip('.')
        if actual_format == 'jpg':
            actual_format = 'jpeg'

        if expected_format != actual_format:
            result['warnings'].append(
                f"File format mismatch: expected {expected_format}, got {actual_format}"
            )

        # Mark as passed if no errors
        result['passed'] = len(result['errors']) == 0

        return result

    def validate_workflow_outputs(self, workflow_file: Path) -> list[dict]:
        """Validate all images generated by a specific workflow."""
        print(f"\nValidating workflow: {workflow_file.name}")

        # Load workflow
        try:
            with open(workflow_file, encoding='utf-8') as f:
                workflow = json.load(f)
        except Exception as e:
            print(f"  ✗ Error loading workflow: {e}")
            return []

        # Extract expected metadata
        expected = WorkflowAnalyzer.extract_expected_metadata(workflow, workflow_file.stem)

        if not expected['has_save_node']:
            print("  ⚠ Warning: No Save Image node found in workflow")
            return []

        print(f"  Expected format: {expected.get('file_format', 'unknown')}")
        if expected.get('sampler_info'):
            print(f"  Sampler nodes found: {len(expected['sampler_info'])}")

        # Find output images
        # The filename_prefix determines the subfolder structure
        # (we search recursively so we don't need to parse the prefix)

        # Search for images in the output folder using a single recursive traversal
        image_suffixes = {'.png', '.jpg', '.jpeg', '.webp'}
        images = [f for f in self.output_dir.rglob("*") if f.is_file() and f.suffix.lower() in image_suffixes]

        if not images:
            print(f"  ⚠ Warning: No output images found in {self.output_dir}")
            return []

        print(f"  Found {len(images)} image(s) to validate")

        # Validate each image
        results = []
        for image_path in images:
            result = self.validate_image(image_path, workflow_file.stem, expected)
            results.append(result)

            # Print result
            status = "✓" if result['passed'] else "✗"
            print(f"    {status} {image_path.name}")

            for error in result['errors']:
                print(f"        Error: {error}")

            for warning in result['warnings']:
                print(f"        Warning: {warning}")

            if result['passed'] and result.get('fields'):
                # Show some key fields
                fields = result['fields']
                if 'Steps' in fields:
                    print(f"        Steps: {fields['Steps']}")
                if 'Sampler' in fields:
                    print(f"        Sampler: {fields['Sampler']}")
                if 'Seed' in fields:
                    print(f"        Seed: {fields['Seed']}")

        return results

    def run_validation(self) -> tuple[int, int, int]:
        """Run validation on all workflows and return (total, passed, failed)."""
        print("=" * 70)
        print("ComfyUI Metadata Validation")
        print("=" * 70)
        print(f"Workflow Dir: {self.workflow_dir}")
        print(f"Output Dir:   {self.output_dir}")
        print("=" * 70)

        if not self.workflow_dir.exists():
            print(f"✗ Error: Workflow directory not found: {self.workflow_dir}")
            return 0, 0, 0

        if not self.output_dir.exists():
            print(f"✗ Error: Output directory not found: {self.output_dir}")
            return 0, 0, 0

        # Find all workflow files
        workflow_files = sorted(self.workflow_dir.glob("*.json"))

        if not workflow_files:
            print(f"⚠ No workflow files found in {self.workflow_dir}")
            return 0, 0, 0

        # Validate each workflow's outputs
        all_results = []
        for workflow_file in workflow_files:
            results = self.validate_workflow_outputs(workflow_file)
            all_results.extend(results)

        # Calculate statistics
        total = len(all_results)
        passed = sum(1 for r in all_results if r['passed'])
        failed = total - passed

        # Print summary
        print("\n" + "=" * 70)
        print("Validation Summary:")
        print(f"  Total Images:  {total}")
        print(f"  ✓ Passed:      {passed}")
        print(f"  ✗ Failed:      {failed}")

        if failed > 0:
            print("\nFailed Images:")
            for result in all_results:
                if not result['passed']:
                    print(f"  - {Path(result['image_path']).name}")
                    for error in result['errors']:
                        print(f"      {error}")

        print("=" * 70)

        self.results = all_results
        return total, passed, failed


def main():
    parser = argparse.ArgumentParser(
        description="Validate metadata in ComfyUI workflow test outputs",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Windows
  python validate_metadata.py ^
    --output-folder "C:\\StableDiffusion\\StabilityMatrix-win-x64\\Data\\Packages\\ComfyUI\\output\\Test"

  # Linux/Mac
  python validate_metadata.py --output-folder "/path/to/ComfyUI/output/Test"

  # Custom workflow directory
  python validate_metadata.py --output-folder "./output/Test" --workflow-dir "./my_workflows"
        """,
    )

    parser.add_argument(
        "--output-folder",
        type=str,
        required=True,
        help="Path to ComfyUI output Test folder containing generated images",
    )

    parser.add_argument(
        "--workflow-dir",
        type=str,
        default="dev_test_workflows",
        help="Directory containing workflow JSON files (default: dev_test_workflows)",
    )

    args = parser.parse_args()

    # Convert to absolute paths
    script_dir = Path(__file__).parent
    workflow_dir = script_dir / args.workflow_dir
    output_dir = Path(args.output_folder)

    # Create validator and run
    validator = MetadataValidator(workflow_dir, output_dir)
    total, passed, failed = validator.run_validation()

    # Exit with appropriate code
    return 0 if failed == 0 and total > 0 else 1


if __name__ == "__main__":
    sys.exit(main())
