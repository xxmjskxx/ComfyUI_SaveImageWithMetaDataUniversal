#!/usr/bin/env python3
"""
Metadata Validation Script for ComfyUI Workflow Test Outputs

This script validates that images generated by ComfyUI workflows contain the expected
metadata based on the workflow configuration. It reads metadata from PNG, JPEG, and WebP
images and compares it against expected values derived from the workflow JSON files.

Usage:
    python validate_metadata.py --output-folder "path/to/output/Test" [options]

Requirements:
    - Pillow (PIL)
    - piexif (for EXIF reading)
"""

import argparse
import atexit
import json
import re
import sys
from pathlib import Path
from typing import Any

TOOLS_DIR = Path(__file__).resolve().parent
TESTS_ROOT = TOOLS_DIR.parent
CLI_COMPAT_DIR = TESTS_ROOT / "comfyui_cli_tests"


def _resolve_relative_path(raw_path: str | None, *, fallback: Path | None = None) -> Path | None:
    """Resolve relative paths against tools/tests compatibility directories."""

    if raw_path is None:
        return None

    candidate = Path(raw_path).expanduser()
    if candidate.is_absolute():
        return candidate

    search_roots = []
    if fallback is not None:
        search_roots.append(fallback)
    search_roots.extend([TOOLS_DIR, TESTS_ROOT, CLI_COMPAT_DIR])

    for root in search_roots:
        resolved = (root / raw_path).resolve()
        if resolved.exists():
            return resolved

    base = fallback or TOOLS_DIR
    return (base / raw_path).resolve()

try:
    from PIL import Image
    from PIL.ExifTags import TAGS
except ImportError:
    print("Error: Pillow is required. Install with: pip install Pillow")
    sys.exit(1)

try:
    import piexif

    PIEXIF_AVAILABLE = True
except ImportError:
    PIEXIF_AVAILABLE = False


class _Tee:
    def __init__(self, stream, log_fp):
        self._stream = stream
        self._log_fp = log_fp

    def write(self, data):
        try:
            self._stream.write(data)
        except (ValueError, AttributeError):
            # Stream might be closed during shutdown
            pass
        try:
            if self._log_fp and not self._log_fp.closed:
                self._log_fp.write(data)
        except (ValueError, AttributeError):
            # Log file might be closed during shutdown
            pass

    def flush(self):
        try:
            self._stream.flush()
        except (ValueError, AttributeError):
            # Stream might be closed during shutdown, ignore flush errors
            pass
        try:
            if self._log_fp and not self._log_fp.closed:
                self._log_fp.flush()
        except (ValueError, AttributeError):
            # Log file might be closed during shutdown, ignore flush errors
            pass


def setup_print_tee(log_file: Path):
    # Ensure parent directory exists
    log_file.parent.mkdir(parents=True, exist_ok=True)
    # Open once; close at exit
    log_fp = open(log_file, "w", encoding="utf-8")

    # Safe close function that handles exceptions during shutdown
    def safe_close():
        try:
            if log_fp and not log_fp.closed:
                log_fp.flush()
                log_fp.close()
        except Exception:
            # Ignore exceptions during shutdown
            pass

    atexit.register(safe_close)

    # Tee both stdout and stderr to the same file
    sys.stdout = _Tee(sys.stdout, log_fp)
    sys.stderr = _Tee(sys.stderr, log_fp)


class MetadataReader:
    """Reads metadata from various image formats."""

    @staticmethod
    def read_png_metadata(image_path: Path) -> dict[str, str]:
        """Read metadata from PNG file."""
        metadata = {}
        try:
            img = Image.open(image_path)

            # Try to get PNG info
            if hasattr(img, "info") and img.info:
                # Look for parameters in PNG metadata
                if "parameters" in img.info:
                    metadata["parameters"] = img.info["parameters"]

                # Copy all text chunks
                for key, value in img.info.items():
                    if isinstance(value, str):
                        metadata[key] = value

            # Alternative: read from binary if structured metadata not found
            if not metadata:
                with open(image_path, "rb") as f:
                    binary_content = f.read()
                    start_index = binary_content.find(b"parameters")
                    if start_index != -1:
                        # Try to extract parameters
                        near_end_index = binary_content.find(b"Hashes: ", start_index)
                        if near_end_index != -1:
                            end_index = binary_content.find(b"tEXt", near_end_index)
                            if end_index != -1:
                                # PNG tEXt chunk has CRC (4 bytes) + length (4 bytes) + type (4 bytes)
                                # We subtract 9 to account for partial chunk structure
                                PNG_TEXT_CHUNK_SUFFIX_LENGTH = 9
                                start_pos = start_index + len(b"parameters") + 1
                                end_pos = end_index - PNG_TEXT_CHUNK_SUFFIX_LENGTH
                                extracted = binary_content[start_pos:end_pos]
                                try:
                                    metadata["parameters"] = extracted.decode("utf-8").strip()
                                except UnicodeDecodeError:
                                    # Decoding failed; skip and continue with other metadata extraction methods
                                    pass
        except Exception as e:
            print(f"  Warning: Error reading PNG metadata from {image_path.name}: {e}")

        return metadata

    @staticmethod
    def decode_user_comment(user_comment: bytes) -> str:
        """Decode EXIF UserComment field."""
        try:
            comment_bytes = user_comment
            # Skip first 8 bytes (character code) if present
            if len(comment_bytes) > 8 and comment_bytes[:8] == b"UNICODE\x00":
                comment_bytes = comment_bytes[8:]
            elif len(comment_bytes) > 4:
                # Try to skip encoding marker
                comment_bytes = comment_bytes[4:]

            # Try UTF-16 BE first
            try:
                return comment_bytes.decode("utf-16be", "backslashreplace")
            except UnicodeDecodeError:
                # UTF-16 BE decoding failed; try other encodings
                pass

            # Try UTF-8
            try:
                return comment_bytes.decode("utf-8", "backslashreplace")
            except UnicodeDecodeError:
                # UTF-8 decoding failed; fall back to latin-1
                pass

            # Fallback to latin-1
            return comment_bytes.decode("latin-1", "replace")
        except Exception:
            return str(user_comment)

    @staticmethod
    def read_jpeg_metadata(image_path: Path) -> dict[str, str]:
        """Read metadata from JPEG file."""
        metadata = {}
        try:
            img = Image.open(image_path)

            # Try using piexif if available
            if PIEXIF_AVAILABLE:
                try:
                    exif_dict = piexif.load(str(image_path))
                    if piexif.ExifIFD.UserComment in exif_dict.get("Exif", {}):
                        user_comment = exif_dict["Exif"][piexif.ExifIFD.UserComment]
                        metadata["parameters"] = MetadataReader.decode_user_comment(user_comment)
                except Exception as e:
                    print(f"  Warning: piexif failed to read EXIF from {image_path.name}: {e}")

            # Fallback to PIL's EXIF reading
            if not metadata and hasattr(img, "_getexif") and img._getexif():
                exif_data = img._getexif()
                for tag, value in exif_data.items():
                    tag_name = TAGS.get(tag, tag)
                    if tag_name == "UserComment" and isinstance(value, bytes):
                        decoded = MetadataReader.decode_user_comment(value)
                        metadata["parameters"] = decoded
                        break

            # Check for JPEG comment marker (fallback mode)
            if not metadata:
                with open(image_path, "rb") as f:
                    content = f.read()
                    # Look for COM marker
                    # JPEG COM (comment) marker is represented by the byte sequence 0xFF 0xFE.
                    # This check detects if the image contains a comment marker, used for fallback metadata.
                    if b"\xff\xfe" in content:
                        metadata["_fallback_mode"] = "com-marker"
        except Exception as e:
            print(f"  Warning: Error reading JPEG metadata from {image_path.name}: {e}")

        return metadata

    @staticmethod
    def read_webp_metadata(image_path: Path) -> dict[str, str]:
        """Read metadata from WebP file."""
        metadata = {}
        try:
            img = Image.open(image_path)

            # WebP can have EXIF data
            if PIEXIF_AVAILABLE and "exif" in img.info:
                try:
                    exif_dict = piexif.load(img.info["exif"])
                    if piexif.ExifIFD.UserComment in exif_dict.get("Exif", {}):
                        user_comment = exif_dict["Exif"][piexif.ExifIFD.UserComment]
                        metadata["parameters"] = MetadataReader.decode_user_comment(user_comment)
                except Exception as exif_error:
                    print(f"  Warning: Error reading EXIF from WebP {image_path.name}: {exif_error}")

            # Check other WebP metadata
            if hasattr(img, "info"):
                for key, value in img.info.items():
                    if isinstance(value, str):
                        metadata[key] = value
        except Exception as e:
            print(f"  Warning: Error reading WebP metadata from {image_path.name}: {e}")

        return metadata

    @staticmethod
    def read_metadata(image_path: Path) -> dict[str, str]:
        """Read metadata from any supported image format."""
        ext = image_path.suffix.lower()

        if ext == ".png":
            return MetadataReader.read_png_metadata(image_path)
        elif ext in [".jpg", ".jpeg"]:
            return MetadataReader.read_jpeg_metadata(image_path)
        elif ext == ".webp":
            return MetadataReader.read_webp_metadata(image_path)
        else:
            print(f"  Warning: Unsupported image format: {ext}")
            return {}


class WorkflowAnalyzer:
    """Analyzes workflow JSON files to extract expected metadata."""

    @staticmethod
    def find_save_nodes(workflow: dict) -> list[tuple[str, dict]]:
        """Find all SaveImageWithMetaDataUniversal nodes in the workflow."""
        save_nodes = []
        for node_id, node_data in workflow.items():
            if node_data.get("class_type") == "SaveImageWithMetaDataUniversal":
                save_nodes.append((node_id, node_data))
        return save_nodes

    @staticmethod
    def find_save_node(workflow: dict) -> tuple[str | None, dict | None]:
        """Find the first Save Image node in the workflow (for backward compatibility)."""
        nodes = WorkflowAnalyzer.find_save_nodes(workflow)
        return nodes[0] if nodes else (None, None)

    @staticmethod
    def find_sampler_nodes(workflow: dict) -> list[tuple[str, dict]]:
        """Find all sampler-like nodes in the workflow."""
        sampler_types = [
            "KSampler",
            "KSamplerAdvanced",
            "SamplerCustom",
            "SamplerCustomAdvanced",
        ]
        samplers = []
        for node_id, node_data in workflow.items():
            if node_data.get("class_type") in sampler_types:
                samplers.append((node_id, node_data))
        return samplers

    @staticmethod
    def resolve_filename_prefix(workflow: dict, filename_prefix: Any) -> str:
        """Resolve filename_prefix which may be a string or a link to another node.

        Args:
            workflow: Dictionary with string keys representing node IDs
            filename_prefix: Either a string or a list [node_id, output_index] linking to another node

        Returns:
            The resolved filename prefix string. Returns empty string if:
            - filename_prefix is a list but the linked node doesn't exist
            - The linked node has no 'value' in its inputs
            - filename_prefix is neither a string nor a list
        """
        if isinstance(filename_prefix, list):
            # It's a link to another node [node_id, output_index]
            link_node_id = str(filename_prefix[0])
            if link_node_id in workflow:
                linked_node = workflow[link_node_id]
                # Get the value from the linked node's inputs
                return linked_node.get("inputs", {}).get("value", "")
        return filename_prefix if isinstance(filename_prefix, str) else ""

    @staticmethod
    def resolve_seed_value(workflow: dict, seed_input: Any) -> str | None:
        """Resolve the expected seed value, including linked seed nodes."""

        if seed_input is None:
            return None

        # Direct literal value
        if not isinstance(seed_input, list):
            if seed_input in (-1, "-1"):
                return "-1"
            return str(seed_input)

        if not seed_input:
            return None

        seed_node_id = str(seed_input[0])
        seed_node = workflow.get(seed_node_id)
        if not seed_node:
            return None

        node_inputs = seed_node.get("inputs", {})

        for key in ("seed", "noise_seed", "value"):
            if key in node_inputs:
                value = node_inputs[key]
                # If the value is another link, we cannot resolve it deterministically
                if isinstance(value, list):
                    return None
                if value in (-1, "-1"):
                    return "-1"
                return str(value)

        # Some seed nodes store the value under "seed_value"
        value = node_inputs.get("seed_value")
        if value is not None:
            if value in (-1, "-1"):
                return "-1"
            return str(value)

        return None

    @staticmethod
    def resolve_guidance_value(workflow: dict, sampler_inputs: dict) -> Any:
        """Resolve guidance (Flux guidance multiplier or CFG equivalent)."""

        direct_guidance = sampler_inputs.get("guidance")
        if direct_guidance is not None:
            return direct_guidance

        stack: list[str] = []
        visited: set[str] = set()

        positive_ref = sampler_inputs.get("positive")
        if isinstance(positive_ref, list) and positive_ref:
            stack.append(str(positive_ref[0]))

        guider_ref = sampler_inputs.get("guider")
        if isinstance(guider_ref, list) and guider_ref:
            stack.append(str(guider_ref[0]))

        while stack:
            node_id = stack.pop()
            if node_id in visited:
                continue
            visited.add(node_id)

            node = workflow.get(node_id)
            if not node:
                continue

            inputs = node.get("inputs", {})
            if inputs.get("guidance") is not None:
                return inputs.get("guidance")

            for key in ("conditioning", "positive", "clip", "input", "guider"):
                ref = inputs.get(key)
                if isinstance(ref, list) and ref:
                    stack.append(str(ref[0]))

        return None

    @staticmethod
    def resolve_text_input(workflow: dict, value: Any, visited: set[str] | None = None) -> str | None:
        """Resolve text-like inputs that may reference other nodes."""

        if value in (None, ""):
            return None

        if isinstance(value, str):
            return value

        if not isinstance(value, list) or not value:
            return None

        node_id = str(value[0])
        if visited is None:
            visited = set()
        if node_id in visited:
            return None
        visited.add(node_id)

        node = workflow.get(node_id)
        if not node:
            return None

        node_inputs = node.get("inputs", {})

        # Direct text-bearing keys
        for key in (
            "value",
            "text",
            "string",
            "clip_l",
            "clip_g",
            "prompt",
            "t5xxl",
            "clip_prompt",
        ):
            direct_value = node_inputs.get(key)
            if isinstance(direct_value, str):
                return direct_value

        # Follow common linkage keys recursively
        for key in ("text", "value", "input", "conditioning", "guider", "clip"):
            linked = node_inputs.get(key)
            if isinstance(linked, list) and linked:
                resolved = WorkflowAnalyzer.resolve_text_input(workflow, linked, visited)
                if resolved:
                    return resolved

        return None

    @staticmethod
    def _resolve_strength_pair(
        inputs: dict,
        *,
        model_default: float = 1.0,
        clip_default: float = 1.0,
    ) -> tuple[float, float]:
        """Normalize LoRA strength fields across loader variants."""

        model_keys = [
            "strength_model",
            "model_strength",
            "lora_model_strength",
            "strength",
            "lora_wt",
        ]
        clip_keys = [
            "strength_clip",
            "clip_strength",
            "lora_clip_strength",
            "strength",
            "lora_wt",
        ]

        model_strength = model_default
        for key in model_keys:
            if key in inputs and inputs[key] not in (None, ""):
                model_strength = inputs[key]
                break

        clip_strength = clip_default
        for key in clip_keys:
            if key in inputs and inputs[key] not in (None, ""):
                clip_strength = inputs[key]
                break

        return model_strength, clip_strength

    @staticmethod
    def _resolve_noise_seed(workflow: dict, noise_ref: Any) -> str | None:
        """Resolve seed from upstream noise nodes."""

        if not (isinstance(noise_ref, list) and noise_ref):
            return None

        node_id = str(noise_ref[0])
        node = workflow.get(node_id)
        if not node:
            return None

        inputs = node.get("inputs", {})
        for key in ("seed", "noise_seed", "value"):
            if inputs.get(key) not in (None, ""):
                value = inputs[key]
                if isinstance(value, list):
                    return None
                return str(value)

        return None

    @staticmethod
    def _resolve_sampler_choice(workflow: dict, sampler_ref: Any) -> str | None:
        """Resolve sampler name from selection nodes like KSamplerSelect."""

        if not (isinstance(sampler_ref, list) and sampler_ref):
            return None

        node = workflow.get(str(sampler_ref[0]))
        if not node:
            return None

        inputs = node.get("inputs", {})
        for key in ("sampler_name", "sampler", "name"):
            value = inputs.get(key)
            if isinstance(value, str) and value:
                return value

        return None

    @staticmethod
    def _resolve_scheduler_metadata(workflow: dict, scheduler_ref: Any) -> dict[str, Any]:
        """Extract step/scheduler details from scheduler helper nodes."""

        metadata: dict[str, Any] = {}
        if not (isinstance(scheduler_ref, list) and scheduler_ref):
            return metadata

        stack = [str(scheduler_ref[0])]
        visited: set[str] = set()

        while stack:
            node_id = stack.pop()
            if node_id in visited:
                continue
            visited.add(node_id)

            node = workflow.get(node_id)
            if not node:
                continue

            inputs = node.get("inputs", {})
            for field in ("steps", "scheduler", "denoise"):
                if field not in metadata and inputs.get(field) not in (None, ""):
                    metadata[field] = inputs[field]

            # Some schedulers reference additional nodes via "model"; follow chain once
            for key in ("model", "scheduler", "sigmas"):
                ref = inputs.get(key)
                if isinstance(ref, list) and ref:
                    stack.append(str(ref[0]))

        return metadata

    @staticmethod
    def resolve_model_hierarchy(workflow: dict, sampler_id: str) -> dict[str, Any]:
        """Trace the sampler's model chain to find checkpoint, LoRAs, and related settings."""

        info: dict[str, Any] = {
            "model_name": None,
            "clip_model_name": None,
            "clip_skip": None,
            "weight_dtype": None,
            "lora_stack": [],
        }

        sampler_node = workflow.get(sampler_id)
        if not sampler_node:
            return info

        queue: list[str] = []
        visited: set[str] = set()

        def enqueue(ref: Any):
            if isinstance(ref, list) and ref:
                node_id = str(ref[0])
                if node_id not in visited:
                    queue.append(node_id)

        for key in ("model", "guider", "sampler", "sigmas", "input"):
            enqueue(sampler_node.get("inputs", {}).get(key))

        while queue:
            node_id = queue.pop(0)
            if node_id in visited:
                continue
            visited.add(node_id)

            node = workflow.get(node_id)
            if not node:
                continue

            class_type = node.get("class_type", "")
            inputs = node.get("inputs", {})
            lower_class = class_type.lower()

            if "lora" in lower_class and inputs.get("lora_name") not in (None, "", "None"):
                model_strength, clip_strength = WorkflowAnalyzer._resolve_strength_pair(inputs)
                info.setdefault("lora_stack", []).append(
                    {
                        "name": inputs.get("lora_name") or inputs.get("name"),
                        "model_strength": model_strength,
                        "clip_strength": clip_strength,
                    }
                )

                enqueue(inputs.get("model"))
                enqueue(inputs.get("unet"))
                enqueue(inputs.get("clip"))
                continue

            checkpoint_keywords = (
                "checkpointloader",
                "model loader",
                "unetloader",
                "fluxunetloader",
                "checkpoint",
            )

            if any(keyword in lower_class for keyword in checkpoint_keywords):
                info["model_name"] = (
                    inputs.get("ckpt_name")
                    or inputs.get("checkpoint_name")
                    or inputs.get("model_name")
                    or inputs.get("unet_name")
                )
                info["clip_model_name"] = (
                    inputs.get("clip_name")
                    or inputs.get("clip_l_name")
                    or inputs.get("clip_g_name")
                )
                info["clip_skip"] = inputs.get("clip_skip", inputs.get("base_clip_skip"))
                info["weight_dtype"] = inputs.get("weight_dtype") or inputs.get("dtype")
                continue

            if class_type in {"ModelSamplingFlux", "ModelSamplingSD3"}:
                info["base_shift"] = inputs.get("base_shift")
                info["max_shift"] = inputs.get("max_shift")
                enqueue(inputs.get("model"))
                enqueue(inputs.get("unet"))
                enqueue(inputs.get("input"))
                enqueue(inputs.get("base_model"))
                continue

            if "cliploader" in lower_class or "dualclip" in lower_class:
                info["clip_model_name"] = (
                    inputs.get("clip_name")
                    or inputs.get("clip_name1")
                    or inputs.get("clip")
                    or info.get("clip_model_name")
                )

            for key in ("model", "unet", "input", "base_model"):
                enqueue(inputs.get(key))

        return info

    @staticmethod
    def resolve_vae_name(workflow: dict, save_node_id: str) -> str | None:
        """Trace from the save node to find the associated VAE loader name."""

        save_node = workflow.get(save_node_id)
        if not save_node:
            return None

        images_ref = save_node.get("inputs", {}).get("images")
        if not (isinstance(images_ref, list) and images_ref):
            return None

        stack = [str(images_ref[0])]
        visited: set[str] = set()

        while stack:
            node_id = stack.pop()
            if node_id in visited:
                continue
            visited.add(node_id)

            node = workflow.get(node_id)
            if not node:
                continue

            class_type = node.get("class_type", "")
            inputs = node.get("inputs", {})

            if class_type in {"VAELoader", "VAELoaderSimple", "FluxVAELoader"}:
                return (
                    inputs.get("vae_name")
                    or inputs.get("clip_vae")
                    or inputs.get("vae_file")
                    or inputs.get("ckpt_name")
                )

            if "vae" in class_type.lower():
                vae_ref = inputs.get("vae")
                if isinstance(vae_ref, list) and vae_ref:
                    stack.append(str(vae_ref[0]))

            for key in ("samples", "latent_image", "images", "input"):
                ref = inputs.get(key)
                if isinstance(ref, list) and ref:
                    stack.append(str(ref[0]))

        return None

    @staticmethod
    def resolve_latent_attributes(workflow: dict, sampler_inputs: dict) -> dict[str, Any]:
        """Derive width, height, batch size by tracing latent sources feeding the sampler."""

        latent_ref = sampler_inputs.get("latent_image") or sampler_inputs.get("latent")
        if not (isinstance(latent_ref, list) and latent_ref):
            return {}

        stack = [str(latent_ref[0])]
        visited: set[str] = set()

        while stack:
            node_id = stack.pop()
            if node_id in visited:
                continue
            visited.add(node_id)

            node = workflow.get(node_id)
            if not node:
                continue

            class_type = node.get("class_type", "")
            inputs = node.get("inputs", {})

            if class_type in {
                "EmptyLatentImage",
                "LatentImage",
                "EmptyLatent",
                "SDXL Empty Latent Image (rgthree)",
            }:
                width = inputs.get("width", inputs.get("empty_latent_width"))
                height = inputs.get("height", inputs.get("empty_latent_height"))
                batch_size = inputs.get("batch_size", inputs.get("batch_count"))

                dimensions = inputs.get("dimensions")
                if (width is None or height is None) and isinstance(dimensions, str):
                    match = re.search(r"(\d+)\s*[xX]\s*(\d+)", dimensions)
                    if match:
                        width = width or int(match.group(1))
                        height = height or int(match.group(2))

                return {
                    "image_width": width,
                    "image_height": height,
                    "batch_size": batch_size,
                }

            for key in ("samples", "latent_image", "input"):
                ref = inputs.get(key)
                if isinstance(ref, list) and ref:
                    stack.append(str(ref[0]))

        return {}

    @staticmethod
    def extract_filename_patterns(workflow: dict) -> list[str]:
        """Extract all filename patterns from SaveImageWithMetaDataUniversal and SaveImage nodes.

        Returns a list of simplified patterns that can be used for matching.
        For example, "Test\\flux-CR-LoRA-stack" becomes "flux-CR-LoRA-stack".
        For complex paths like "Test\\siwm-%model:10%/%pprompt:20%", extracts "siwm".
        """
        patterns = []
        seen_patterns = set()  # Track unique patterns

        # Find SaveImageWithMetaDataUniversal nodes
        save_nodes = WorkflowAnalyzer.find_save_nodes(workflow)

        # Also find regular SaveImage nodes (for control images)
        for node_id, node_data in workflow.items():
            if node_data.get("class_type") == "SaveImage":
                save_nodes.append((node_id, node_data))

        for _, save_node in save_nodes:
            inputs = save_node.get("inputs", {})
            prefix = inputs.get("filename_prefix", "")

            # Resolve linked filename_prefix
            prefix = WorkflowAnalyzer.resolve_filename_prefix(workflow, prefix)

            if prefix:
                # Extract the meaningful part of the pattern
                # Remove path separators and tokens like %date%, %seed%, etc.
                # Keep the static parts that identify the workflow

                # Split by path separators (both / and \)
                parts = prefix.replace("\\", "/").split("/")

                for part in parts:
                    # Remove common tokens but keep the base name
                    # Strip leading/trailing spaces
                    clean_part = part.strip()

                    # Skip empty parts
                    if not clean_part:
                        continue

                    # Skip parts that are ONLY tokens
                    if clean_part.startswith("%") and clean_part.endswith("%"):
                        continue

                    # Remove token patterns but keep the static text
                    # For example: "siwm-%model:10%" -> "siwm"
                    # For example: "%date:yyyy-MM-dd-hhmmss%-Flux-dual-clip" -> "Flux-dual-clip"
                    # Remove all %...% patterns
                    cleaned = re.sub(r"%[^%]+%", "", clean_part)
                    # Remove dimension separators (x between width/height tokens leaves stray "x")
                    # This handles cases like "%width%x%height%" -> "x" -> ""
                    cleaned = re.sub(r"^[x_\-/]+", "", cleaned)  # Remove leading separators and x
                    cleaned = re.sub(r"[x_\-/]+$", "", cleaned)  # Remove trailing separators and x
                    # Final cleanup: collapse multiple separators but preserve their type
                    # Don't normalize dashes to underscores - keep original structure
                    cleaned = re.sub(r"_+", "_", cleaned)  # Collapse multiple underscores
                    cleaned = re.sub(r"-+", "-", cleaned)  # Collapse multiple dashes
                    cleaned = cleaned.strip("_-")  # Strip leading/trailing separators

                    # Only add non-generic patterns (not just "Test" or "Tests")
                    # Require minimum length of 3 to avoid overly broad matches like "a" or "xy"
                    # Use case-insensitive deduplication to match case-insensitive matching logic
                    cleaned_lower = cleaned.lower()
                    if (
                        cleaned
                        and len(cleaned) >= 3
                        and cleaned_lower not in {"test", "tests"}
                        and cleaned_lower not in seen_patterns
                    ):
                        patterns.append(cleaned)
                        seen_patterns.add(cleaned_lower)

        return patterns

    @staticmethod
    def trace_node_input(workflow: dict, node_id: str, input_key: str) -> tuple[str | None, dict | None]:
        """Trace back through a node input connection to find the source node.

        Returns: (source_node_id, source_node) or (None, None) if not found
        """
        if node_id not in workflow:
            return None, None

        node = workflow[node_id]
        inputs = node.get("inputs", {})

        if input_key not in inputs:
            return None, None

        value = inputs[input_key]
        # If it's a list, it's a connection [node_id, output_index]
        if isinstance(value, list) and len(value) >= 1:
            source_id = str(value[0])
            if source_id in workflow:
                return source_id, workflow[source_id]

        return None, None

    @staticmethod
    def extract_lora_stack_info(workflow: dict, lora_stack_id: str) -> list[dict]:
        """Extract LoRA information from a LoRA Stacker node.

        Returns: List of dicts with {name, model_strength, clip_strength}
        """
        if lora_stack_id not in workflow:
            return []

        lora_stack_node = workflow[lora_stack_id]
        stack_inputs = lora_stack_node.get("inputs", {})

        mode = stack_inputs.get("input_mode", "simple")
        count = int(stack_inputs.get("lora_count", 0))

        loras = []
        for i in range(1, count + 1):
            name_key = f'lora_name_{i}'
            lora_name = stack_inputs.get(name_key)

            if lora_name and lora_name != "None":
                if mode == "advanced":
                    model_str = stack_inputs.get(f'model_str_{i}', 1.0)
                    clip_str = stack_inputs.get(f'clip_str_{i}', 1.0)
                else:
                    lora_wt = stack_inputs.get(f'lora_wt_{i}', 1.0)
                    model_str = lora_wt
                    clip_str = lora_wt

                loras.append({
                    "name": lora_name,
                    "model_strength": model_str,
                    "clip_strength": clip_str
                })

        return loras

    @staticmethod
    def find_selected_sampler(
        workflow: dict,
        selection_method: str,
        selection_node_id: str | None,
    ) -> tuple[str | None, dict | None]:
        """Return the sampler node selected by the configured method."""
        # Find all KSampler nodes
        samplers = []
        for node_id, node_data in workflow.items():
            class_type = node_data.get("class_type", "")
            if "KSampler" in class_type or "SamplerCustom" in class_type:
                steps = node_data.get("inputs", {}).get("steps")
                if steps is not None:
                    samplers.append((node_id, node_data, int(steps)))

        if not samplers:
            return None, None

        if selection_method == "By node ID" and selection_node_id:
            # Find specific node
            for node_id, node_data, _ in samplers:
                if node_id == str(selection_node_id):
                    return node_id, node_data
            return None, None

        elif selection_method == "Farthest":
            # Find sampler with highest steps
            sampler_id, sampler_node, _ = max(samplers, key=lambda x: x[2])
            return sampler_id, sampler_node

        elif selection_method == "Nearest":
            # Find sampler with lowest steps
            sampler_id, sampler_node, _ = min(samplers, key=lambda x: x[2])
            return sampler_id, sampler_node

        # Default: return first sampler found
        return samplers[0][0], samplers[0][1]

    @staticmethod
    def extract_expected_metadata_for_save_node(workflow: dict, save_node_id: str, save_node: dict) -> dict[str, Any]:
        """Extract complete expected metadata for a specific Save Image node by tracing its connections.

        Returns a dict with all expected metadata fields including:
        - Basic save node settings (file_format, etc.)
        - Sampler parameters (seed, steps, cfg, sampler_name, scheduler, denoise)
        - Loader parameters (model, vae, clip_skip, dimensions, prompts)
        - LoRA information (names, strengths)
        """
        expected = {
            "save_node_id": save_node_id,
            "filename_prefix": "",
            "filename_patterns": [],
            "file_format": "png",
        }

        # Get save node settings
        save_inputs = save_node.get("inputs", {})
        expected["filename_prefix"] = WorkflowAnalyzer.resolve_filename_prefix(
            workflow, save_inputs.get("filename_prefix", "")
        )

        # Extract filename patterns from this specific save node's prefix
        if expected["filename_prefix"]:
            prefix = expected["filename_prefix"]
            parts = prefix.replace("\\", "/").split("/")
            for part in parts:
                clean_part = part.strip()
                if not clean_part or (clean_part.startswith("%") and clean_part.endswith("%")):
                    continue
                # Remove token patterns but keep static text
                cleaned = re.sub(r"%[^%]+%", "", clean_part)
                # Remove dimension separators
                cleaned = re.sub(r"^[x_\-/]+", "", cleaned)
                cleaned = re.sub(r"[x_\-/]+$", "", cleaned)
                # Collapse multiple separators but preserve their type
                cleaned = re.sub(r"_+", "_", cleaned)
                cleaned = re.sub(r"-+", "-", cleaned)
                cleaned = cleaned.strip("_-")
                # Only add meaningful patterns
                cleaned_lower = cleaned.lower()
                if cleaned and len(cleaned) >= 3 and cleaned_lower not in {"test", "tests"}:
                    expected["filename_patterns"].append(cleaned)

        expected["file_format"] = save_inputs.get("file_format", "png")
        expected["save_workflow_json"] = save_inputs.get("save_workflow_json", False)
        expected["include_lora_summary"] = save_inputs.get("include_lora_summary", False)
        expected["max_jpeg_exif_kb"] = save_inputs.get("max_jpeg_exif_kb", 60)
        expected["guidance_as_cfg"] = save_inputs.get("guidance_as_cfg", False)
        expected["civitai_sampler"] = save_inputs.get("civitai_sampler", False)
        expected["sampler_selection_method"] = save_inputs.get("sampler_selection_method", "Farthest")
        expected["sampler_selection_node_id"] = save_inputs.get("sampler_selection_node_id")
        expected["include_lora_summary"] = save_inputs.get("include_lora_summary", False)

        # Trace to sampler (may need to trace through intermediate nodes like VAEDecode)
        sampler_id, sampler_node = WorkflowAnalyzer.trace_node_input(workflow, save_node_id, "images")

        traced_sampler_is_valid = bool(
            sampler_node and "Sampler" in sampler_node.get("class_type", "")
        )

        # Check if we should use sampler selection method instead
        selection_method = expected.get("sampler_selection_method", "Farthest")
        selection_node_id = expected.get("sampler_selection_node_id")

        # Count samplers in workflow
        sampler_count = sum(
            1
            for nid, nd in workflow.items()
            if "KSampler" in nd.get("class_type", "")
        )

        # If multiple samplers exist, use selection method only when needed
        if sampler_count > 1:
            should_use_selection = False
            if not sampler_node:
                should_use_selection = True
            elif selection_method == "By node ID" and selection_node_id:
                should_use_selection = True
            elif not traced_sampler_is_valid:
                should_use_selection = True

            if should_use_selection:
                selected_sampler_id, selected_sampler_node = WorkflowAnalyzer.find_selected_sampler(
                    workflow, selection_method, selection_node_id
                )
                if selected_sampler_node:
                    sampler_id = selected_sampler_id
                    sampler_node = selected_sampler_node
                    expected["sampler_node_id"] = sampler_id
                    expected["sampler_class_type"] = sampler_node.get("class_type")
        if not sampler_node:
            return expected

        expected["sampler_node_id"] = sampler_id
        expected["sampler_class_type"] = sampler_node.get("class_type")

        # If we hit a VAEDecode or other intermediate node, trace back to find the actual sampler
        intermediate_nodes = ["VAEDecode", "VAEEncode", "ImageScale", "LatentUpscale"]
        if sampler_node.get("class_type") in intermediate_nodes:
            # Trace back through the 'samples' or 'latent_image' input
            for input_key in ["samples", "latent_image"]:
                actual_sampler_id, actual_sampler_node = WorkflowAnalyzer.trace_node_input(
                    workflow, sampler_id, input_key
                )
                if actual_sampler_node:
                    sampler_id = actual_sampler_id
                    sampler_node = actual_sampler_node
                    expected["sampler_node_id"] = sampler_id
                    expected["sampler_class_type"] = sampler_node.get("class_type")
                    break

        # Extract sampler parameters
        sampler_inputs = sampler_node.get("inputs", {})
        seed_input = sampler_inputs.get("seed", sampler_inputs.get("noise_seed"))
        expected["seed"] = WorkflowAnalyzer.resolve_seed_value(workflow, seed_input)
        if expected["seed"] is None:
            expected["seed"] = WorkflowAnalyzer._resolve_noise_seed(workflow, sampler_inputs.get("noise"))
        if expected["seed"] is not None:
            expected["seed"] = str(expected["seed"])
        expected["steps"] = sampler_inputs.get("steps")
        expected["cfg"] = sampler_inputs.get("cfg")
        expected["sampler_name"] = sampler_inputs.get("sampler_name")
        expected["scheduler"] = sampler_inputs.get("scheduler")
        expected["denoise"] = sampler_inputs.get("denoise")
        expected["guidance"] = WorkflowAnalyzer.resolve_guidance_value(workflow, sampler_inputs)

        scheduler_meta = WorkflowAnalyzer._resolve_scheduler_metadata(workflow, sampler_inputs.get("sigmas"))
        for key in ("steps", "scheduler", "denoise"):
            if scheduler_meta.get(key) is not None and expected.get(key) in (None, ""):
                expected[key] = scheduler_meta[key]

        if not expected.get("sampler_name"):
            sampler_choice = WorkflowAnalyzer._resolve_sampler_choice(workflow, sampler_inputs.get("sampler"))
            if sampler_choice:
                expected["sampler_name"] = sampler_choice

        if sampler_inputs.get("positive"):
            expected["positive_prompt"] = WorkflowAnalyzer.resolve_text_input(
                workflow, sampler_inputs.get("positive")
            )
        if expected.get("positive_prompt") is None:
            expected["positive_prompt"] = WorkflowAnalyzer.resolve_text_input(
                workflow,
                sampler_inputs.get("guider"),
            )

        if sampler_inputs.get("negative"):
            expected["negative_prompt"] = WorkflowAnalyzer.resolve_text_input(
                workflow, sampler_inputs.get("negative")
            )
        if expected.get("negative_prompt") is None:
            expected["negative_prompt"] = WorkflowAnalyzer.resolve_text_input(
                workflow,
                sampler_inputs.get("conditioning")
                if sampler_inputs.get("conditioning") not in (None, "")
                else None,
            )

        # Trace to loader (could be through 'model' or 'sdxl_tuple' input)
        loader_id, loader_node = None, None
        for input_key in ["model", "sdxl_tuple"]:
            loader_id, loader_node = WorkflowAnalyzer.trace_node_input(workflow, sampler_id, input_key)
            if loader_node:
                break

        if loader_node:
            expected["loader_node_id"] = loader_id
            expected["loader_class_type"] = loader_node.get("class_type")

            loader_inputs = loader_node.get("inputs", {})

            loader_positive = WorkflowAnalyzer.resolve_text_input(workflow, loader_inputs.get("positive"))
            if loader_positive:
                expected["positive_prompt"] = loader_positive

            loader_negative = WorkflowAnalyzer.resolve_text_input(workflow, loader_inputs.get("negative"))
            if loader_negative:
                expected["negative_prompt"] = loader_negative

            # Inline LoRA (for workflows using loader-level LoRA blend)
            inline_lora = loader_inputs.get("lora_name")
            if inline_lora and inline_lora != "None":
                expected["inline_lora"] = {
                    "name": inline_lora,
                    "model_strength": loader_inputs.get(
                        "lora_model_strength", loader_inputs.get("lora_wt", 1.0)
                    ),
                    "clip_strength": loader_inputs.get(
                        "lora_clip_strength", loader_inputs.get("lora_wt", 1.0)
                    ),
                }

            # T5/CLIP prompts for dual CLIP
            expected["t5_prompt"] = WorkflowAnalyzer.resolve_text_input(
                workflow, loader_inputs.get("t5xxl")
            )
            expected["clip_prompt"] = WorkflowAnalyzer.resolve_text_input(
                workflow, loader_inputs.get("clip_l")
            )

            # LoRA stack node reference
            lora_stack_ref = loader_inputs.get("lora_stack")
            if isinstance(lora_stack_ref, list) and len(lora_stack_ref) >= 1:
                lora_stack_id = str(lora_stack_ref[0])
                expected["lora_stack"] = WorkflowAnalyzer.extract_lora_stack_info(workflow, lora_stack_id)

        model_info = WorkflowAnalyzer.resolve_model_hierarchy(workflow, sampler_id)

        if model_info.get("model_name"):
            expected["model_name"] = model_info["model_name"]
        if model_info.get("clip_model_name"):
            expected["clip_model_name"] = model_info["clip_model_name"]
        if model_info.get("clip_skip") is not None:
            expected["clip_skip"] = model_info["clip_skip"]
        if model_info.get("weight_dtype"):
            expected["weight_dtype"] = model_info["weight_dtype"]
        if model_info.get("lora_stack"):
            expected["lora_stack"] = model_info["lora_stack"]
            if not expected.get("include_lora_summary"):
                expected["include_lora_summary"] = True

        if model_info.get("base_shift") is not None:
            expected["base_shift"] = model_info["base_shift"]
        if model_info.get("max_shift") is not None:
            expected["max_shift"] = model_info["max_shift"]

        vae_name = WorkflowAnalyzer.resolve_vae_name(workflow, save_node_id)
        if vae_name:
            expected["vae_name"] = vae_name

        latent_attrs = WorkflowAnalyzer.resolve_latent_attributes(workflow, sampler_inputs)
        if latent_attrs:
            if latent_attrs.get("image_width"):
                expected["image_width"] = latent_attrs["image_width"]
            if latent_attrs.get("image_height"):
                expected["image_height"] = latent_attrs["image_height"]
            if latent_attrs.get("batch_size") is not None:
                expected["batch_size"] = latent_attrs["batch_size"]

        return expected

    @staticmethod
    def extract_expected_metadata(workflow: dict, workflow_name: str) -> dict[str, Any]:
        """Extract expected metadata fields from workflow.

        Returns a dictionary with:
        - workflow_name: Name of the workflow file
        - has_save_node: Whether a SaveImageWithMetaDataUniversal node exists
        - save_nodes: List of detailed expected metadata for each save node
        - filename_patterns: List of patterns extracted from filename_prefix
        """
        expected = {
            "workflow_name": workflow_name,
            "has_save_node": False,
            "save_nodes": [],
            "filename_patterns": [],
        }

        # Find save nodes (there can be multiple)
        save_nodes = WorkflowAnalyzer.find_save_nodes(workflow)
        if save_nodes:
            expected["has_save_node"] = True

            # Extract complete metadata expectations for each save node
            for save_node_id, save_node in save_nodes:
                save_node_expected = WorkflowAnalyzer.extract_expected_metadata_for_save_node(
                    workflow, save_node_id, save_node
                )
                expected["save_nodes"].append(save_node_expected)

            # Extract filename patterns from all save nodes
            expected["filename_patterns"] = WorkflowAnalyzer.extract_filename_patterns(workflow)

        return expected


class MetadataValidator:
    """Validates image metadata against expected values."""

    # Compiled regex patterns for known metadata key patterns (compiled once for efficiency)
    KNOWN_KEY_PATTERNS = [
        re.compile(r"^Lora_\d+$"),
        re.compile(r"^Lora_\d+\s+.+$"),  # Matches "Lora_0 Model name", "Lora_0 Model hash", etc.
        re.compile(r"^Embedding_\d+$"),
        re.compile(r"^Embedding_\d+\s+.+$"),  # Matches "Embedding_0 name", "Embedding_0 hash", etc.
        re.compile(r"^CLIP_\d+\s+.+$"),  # Matches "CLIP_1 Model name", etc.
    ]

    def __init__(
        self,
        workflow_dir: Path,
        output_dir: Path,
        comfyui_models_path: Path | None = None,
        verbose: bool = False,
    ):
        self.workflow_dir = workflow_dir
        self.output_dir = output_dir
        self.comfyui_models_path = comfyui_models_path
        self.results = []
        self.verbose = verbose

    def _extract_json_value(self, text: str) -> str:
        """Extract JSON object or array from the beginning of text.

        This handles cases where JSON is followed by other metadata fields.
        For example: '{"model": "abc"}, custom_field: value' -> '{"model": "abc"}'
        """
        text = text.strip()
        if not text:
            return text

        # Track brace/bracket depth to find where JSON ends
        if text[0] == "{":
            depth = 0
            in_string = False
            escape_next = False

            for i, char in enumerate(text):
                if escape_next:
                    escape_next = False
                    continue

                if char == "\\":
                    escape_next = True
                    continue

                if char == '"' and not escape_next:
                    in_string = not in_string
                    continue

                if not in_string:
                    if char == "{":
                        depth += 1
                    elif char == "}":
                        depth -= 1
                        if depth == 0:
                            # Found the end of the JSON object
                            return text[: i + 1]

        elif text[0] == "[":
            depth = 0
            in_string = False
            escape_next = False

            for i, char in enumerate(text):
                if escape_next:
                    escape_next = False
                    continue

                if char == "\\":
                    escape_next = True
                    continue

                if char == '"' and not escape_next:
                    in_string = not in_string
                    continue

                if not in_string:
                    if char == "[":
                        depth += 1
                    elif char == "]":
                        depth -= 1
                        if depth == 0:
                            # Found the end of the JSON array
                            return text[: i + 1]

        # If not JSON or couldn't parse, return as-is
        return text

    def parse_parameters_string(self, params_str: str) -> dict[str, str]:
        """Parse the parameters string into a dictionary of fields.

        Handles both formats:
        1. Single-line format: "key: value, key: value, ..."
        2. Multi-line format: "key: value\\nkey: value\\n..."

        Only treats text as key-value pairs if the key matches a known metadata key pattern.
        This prevents incorrectly splitting prompt text containing colons (e.g., "at 3:00 PM" or URLs).
        """
        if not params_str:
            return {}

        # Define known metadata keys that should be treated as field names
        # These are the standard metadata fields from the Save Image node
        known_keys = {
            "Steps",
            "Sampler",
            "CFG scale",
            "Seed",
            "Size",
            "Model",
            "Model hash",
            "VAE",
            "VAE hash",
            "Clip skip",
            "Denoise",
            "Shift",
            "Max shift",
            "Base shift",
            "Guidance",
            "Scheduler",
            "Hashes",
            "Metadata generator version",
            "Batch index",
            "Batch size",
            "Metadata Fallback",
            "LoRAs",
            "Weight dtype",
            "Samplers",
            "T5 Prompt",
            "CLIP Prompt",
            "CLIP_1 Model name",
            "CLIP_2 Model name",
        }

        # Keys that typically appear in the prompt header (before metadata)
        prompt_header_keys = {"T5 Prompt", "CLIP Prompt", "Positive prompt", "Negative prompt"}

        # Keys that signal the start of actual metadata (not prompts)
        metadata_start_keys = known_keys - prompt_header_keys

        fields = {}

        # First, extract the prompt header (positive and negative prompts)
        # These appear at the start before the metadata fields
        lines = params_str.strip().split("\n")
        metadata_start_idx = 0

        # Look for where the actual metadata starts (after prompts)
        # Metadata fields start with known metadata keys (not prompt headers) followed by a colon
        for idx, line in enumerate(lines):
            match = re.match(r"^([A-Za-z0-9 _\-]+):\s*(.*)", line)
            if match:
                potential_key = match.group(1).strip()
                is_metadata_key = potential_key in metadata_start_keys or any(
                    pattern.match(potential_key) for pattern in self.KNOWN_KEY_PATTERNS
                )
                if is_metadata_key:
                    metadata_start_idx = idx
                    break

        # Capture prompt headers up to metadata start
        header_fields: dict[str, str] = {}
        current_header_key: str | None = None
        positive_lines: list[str] = []
        for line in lines[:metadata_start_idx]:
            stripped_line = line.strip()
            match = re.match(r"^([A-Za-z0-9 _\-]+):\s*(.*)", line)
            if match:
                potential_key = match.group(1).strip()
                if potential_key in prompt_header_keys:
                    header_fields[potential_key] = match.group(2).strip()
                    current_header_key = potential_key
                else:
                    current_header_key = None
                continue

            if current_header_key and stripped_line:
                header_fields[current_header_key] = "\n".join(
                    filter(None, [header_fields[current_header_key], stripped_line])
                )
            elif stripped_line:
                positive_lines.append(stripped_line)

        if positive_lines and "Positive prompt" not in header_fields:
            header_fields["Positive prompt"] = "\n".join(positive_lines).strip()

        fields.update(header_fields)

        # Join metadata lines (after prompts)
        if metadata_start_idx < len(lines):
            metadata_text = "\n".join(lines[metadata_start_idx:])
        else:
            metadata_text = params_str

        # Detect format: check if we have comma separators between known keys
        # Build combined regex patterns for comma and newline formats (more efficient)
        escaped_keys = [re.escape(k) for k in known_keys]
        comma_pattern = re.compile(r",\s*(?:" + "|".join(escaped_keys) + r"):")
        newline_pattern = re.compile(r"\n\s*(?:" + "|".join(escaped_keys) + r"):")

        comma_key_count = len(comma_pattern.findall(metadata_text))
        newline_key_count = len(newline_pattern.findall(metadata_text))

        # Determine which format to use
        use_comma_format = comma_key_count > newline_key_count

        if use_comma_format:
            # Parse comma-separated format
            # Split by commas, but only if followed by a known key or Lora_/Embedding_ pattern
            # Build a regex that matches ", known_key:" patterns

            # Create a list of all known key patterns
            all_patterns = list(known_keys)

            # Build the split pattern dynamically
            escaped_keys = [re.escape(k) + r":" for k in all_patterns]
            split_pattern = r",\s*(?=" + "|".join(escaped_keys) + r"|(?:Lora_|Embedding_|CLIP_)\d+\s+[^:]+:)"

            parts = re.split(split_pattern, metadata_text)

            for part in parts:
                part = part.strip()
                if not part:
                    continue

                # Extract key: value from this part
                # Allow spaces in keys for patterns like "Lora_0 Model name"
                match = re.match(r"^([A-Za-z0-9 _\-]+):\s*(.*)", part, re.DOTALL)
                if match:
                    key = match.group(1).strip()
                    value = match.group(2).strip()

                    # Special handling for Hashes field: extract just the JSON portion
                    if key == "Hashes":
                        value = self._extract_json_value(value)

                    # Verify it's a known key or matches a pattern
                    is_known = key in known_keys or any(pattern.match(key) for pattern in self.KNOWN_KEY_PATTERNS)
                    if is_known:
                        fields[key] = value
        else:
            # Parse newline-separated format (original logic)
            current_key = None
            current_value = []

            for line in lines[metadata_start_idx:]:
                # Check if line starts with a known key followed by a colon
                match = re.match(r"^([A-Za-z0-9 _\-]+):\s*(.*)", line)
                if match:
                    potential_key = match.group(1).strip()
                    value = match.group(2)

                    # Check if this is a known metadata key
                    is_known_key = potential_key in known_keys
                    if not is_known_key:
                        # Check against compiled patterns (e.g., "Lora_1", "Embedding_2 hash")
                        for pattern in self.KNOWN_KEY_PATTERNS:
                            if pattern.match(potential_key):
                                is_known_key = True
                                break

                    if is_known_key:
                        # Save previous key-value if exists
                        if current_key:
                            fields[current_key] = "\n".join(current_value).strip()

                        current_key = potential_key
                        current_value = []
                        if value:
                            current_value.append(value)
                        continue

                # If we get here, this line is either a continuation or not a field
                if current_key:
                    # Continuation of previous value
                    if line.strip():
                        current_value.append(line.strip())

            # Save last key-value
            if current_key:
                fields[current_key] = "\n".join(current_value).strip()

        return fields

    def _validate_expected_fields(self, fields: dict, expected_metadata: dict, result: dict):
        """Comprehensively validate that actual metadata matches all expected values."""

        check_details = result.setdefault("check_details", [])
        recorded_fields: set[str] = set()

        def normalize_value(value: Any) -> str:
            if value is None or value == "":
                return "N/A"
            return str(value)

        def normalize_text(value: Any) -> str:
            if value is None or value == "":
                return ""
            if isinstance(value, str):
                return value.strip()
            return str(value).strip()

        def add_detail(
            field_name: str,
            status: str,
            expected: Any = None,
            actual: Any = None,
            message: str | None = None,
        ):
            detail: dict[str, Any] = {"field": field_name, "status": status}
            if expected is not None:
                detail["expected"] = expected
            if actual is not None:
                detail["actual"] = actual
            if message:
                detail["message"] = message
            check_details.append(detail)
            if status != "info":
                recorded_fields.add(field_name)

        def mark_pass(field_name: str, expected: Any, actual: Any):
            add_detail(field_name, "pass", expected, actual)

        def mark_fail(field_name: str, expected: Any, actual: Any, message: str):
            result["errors"].append(message)
            add_detail(field_name, "fail", expected, actual, message)

        def mark_warn(field_name: str, expected: Any, actual: Any, message: str):
            result["warnings"].append(message)
            add_detail(field_name, "warn", expected, actual, message)

        def compare_numeric_field(field_name: str, expected: Any):
            expected_str = str(expected)
            actual_value = fields.get(field_name)
            actual_str = normalize_value(actual_value)
            if actual_value in (None, ""):
                message = f"{field_name} missing, expected '{expected_str}'"
                mark_fail(field_name, expected_str, actual_str, message)
                return
            try:
                expected_float = float(expected_str)
                actual_float = float(str(actual_value))
                if abs(expected_float - actual_float) > 0.0001:
                    message = (
                        f"{field_name} mismatch: expected '{expected_str}', "
                        f"got '{actual_str}'"
                    )
                    mark_fail(field_name, expected_str, actual_str, message)
                else:
                    mark_pass(field_name, expected_str, actual_str)
            except (ValueError, TypeError):
                if str(actual_value) != expected_str:
                    message = (
                        f"{field_name} mismatch: expected '{expected_str}', "
                        f"got '{actual_str}'"
                    )
                    mark_fail(field_name, expected_str, actual_str, message)
                else:
                    mark_pass(field_name, expected_str, actual_str)

        def compare_string_field(field_name: str, expected: Any):
            expected_str = normalize_text(expected)
            actual_value = fields.get(field_name)
            actual_str = normalize_text(actual_value)
            if actual_value in (None, ""):
                message = f"{field_name} missing, expected '{expected_str or 'value'}'"
                mark_fail(field_name, expected_str or "value", normalize_value(actual_value), message)
            elif actual_str != expected_str:
                message = f"{field_name} mismatch: expected '{expected_str}', got '{actual_str}'"
                mark_fail(field_name, expected_str, actual_str, message)
            else:
                mark_pass(field_name, expected_str, actual_str)

        def ensure_presence(field_name: str, expected_value: Any | None = None, *, message: str | None = None):
            if field_name in recorded_fields:
                return field_name in fields and fields[field_name] not in (None, "")
            actual_value = fields.get(field_name)
            actual_str = normalize_value(actual_value)
            if actual_value not in (None, ""):
                expected_str = (
                    normalize_value(expected_value)
                    if expected_value not in (None, "")
                    else "present"
                )
                recorded_fields.add(field_name)
                add_detail(field_name, "pass", expected=expected_str, actual=actual_str)
                return True
            expected_str = (
                normalize_value(expected_value)
                if expected_value not in (None, "")
                else "present"
            )
            msg = message or f"{field_name} missing"
            mark_fail(field_name, expected_str, actual_str, msg)
            return False

        # Validate seed
        if expected_metadata.get("seed") is not None:
            expected_seed = str(expected_metadata["seed"])
            actual_seed = fields.get("Seed", "")
            actual_seed_str = normalize_value(actual_seed)
            if expected_seed == "-1":
                if actual_seed and len(str(actual_seed)) == 15 and str(actual_seed).isdigit():
                    mark_pass("Seed", "15-digit random", actual_seed_str)
                else:
                    mark_fail(
                        "Seed",
                        "15-digit random",
                        actual_seed_str,
                        f"Seed format mismatch: expected 15-digit random seed, got '{actual_seed_str}'",
                    )
            else:
                compare_numeric_field("Seed", expected_seed)

        # Validate steps
        if expected_metadata.get("steps") is not None:
            compare_numeric_field("Steps", expected_metadata["steps"])

        # Validate CFG (respect guidance_as_cfg toggle)
        if expected_metadata.get("cfg") is not None:
            cfg_expected = expected_metadata["cfg"]
            if expected_metadata.get("guidance_as_cfg") and expected_metadata.get("guidance") is not None:
                cfg_expected = expected_metadata["guidance"]
            compare_numeric_field("CFG scale", cfg_expected)

        # Validate sampler name (scheduler may transform display)
        if expected_metadata.get("sampler_name"):
            expected_sampler = str(expected_metadata["sampler_name"])
            actual_sampler = fields.get("Sampler")
            actual_sampler_str = normalize_value(actual_sampler)
            if actual_sampler:
                if expected_sampler.lower() in actual_sampler_str.lower():
                    mark_pass("Sampler", expected_sampler, actual_sampler_str)
                else:
                    mark_fail(
                        "Sampler",
                        expected_sampler,
                        actual_sampler_str,
                        f"Sampler mismatch: expected '{expected_sampler}', got '{actual_sampler_str}'",
                    )
            else:
                mark_fail(
                    "Sampler",
                    expected_sampler,
                    "N/A",
                    f"Sampler field missing, expected sampler '{expected_sampler}'",
                )

        if expected_metadata.get("scheduler"):
            expected_scheduler = str(expected_metadata["scheduler"])
            actual_sampler = fields.get("Sampler")
            actual_sampler_str = normalize_value(actual_sampler)
            if actual_sampler:
                if expected_scheduler.lower() in actual_sampler_str.lower():
                    mark_pass("Scheduler", expected_scheduler, actual_sampler_str)
                else:
                    mark_fail(
                        "Scheduler",
                        expected_scheduler,
                        actual_sampler_str,
                        f"Scheduler mismatch: expected '{expected_scheduler}', got '{actual_sampler_str}'",
                    )
            else:
                mark_fail(
                    "Scheduler",
                    expected_scheduler,
                    "N/A",
                    f"Sampler field missing, expected scheduler '{expected_scheduler}'",
                )

        # Validate denoise and guidance
        if expected_metadata.get("denoise") is not None:
            compare_numeric_field("Denoise", expected_metadata["denoise"])

        if expected_metadata.get("guidance") is not None:
            compare_numeric_field("Guidance", expected_metadata["guidance"])

        # Validate model name (basename only)
        if expected_metadata.get("model_name"):
            actual_model = fields.get("Model")
            expected_model_path = str(expected_metadata["model_name"]).replace("\\", "/")
            expected_model_basename = Path(expected_model_path).stem
            actual_model_basename = Path(actual_model).stem if actual_model else ""
            if actual_model_basename:
                if actual_model_basename == expected_model_basename:
                    mark_pass("Model", expected_model_basename, actual_model_basename)
                else:
                    mark_fail(
                        "Model",
                        expected_model_basename,
                        actual_model_basename,
                        f"Model mismatch: expected '{expected_model_basename}', got '{actual_model_basename}'",
                    )
            else:
                message = f"Model missing, expected '{expected_model_basename}'"
                mark_fail("Model", expected_model_basename, "N/A", message)

        # Validate VAE name
        if expected_metadata.get("vae_name") and expected_metadata["vae_name"] != "Baked VAE":
            actual_vae = fields.get("VAE")
            expected_vae_basename = Path(str(expected_metadata["vae_name"])).stem
            actual_vae_basename = Path(actual_vae).stem if actual_vae else ""
            if actual_vae_basename:
                if actual_vae_basename == expected_vae_basename:
                    mark_pass("VAE", expected_vae_basename, actual_vae_basename)
                else:
                    mark_fail(
                        "VAE",
                        expected_vae_basename,
                        actual_vae_basename,
                        f"VAE mismatch: expected '{expected_vae_basename}', got '{actual_vae_basename}'",
                    )
            else:
                mark_fail("VAE", expected_vae_basename, "N/A", f"VAE missing, expected '{expected_vae_basename}'")

        # Validate clip skip
        if expected_metadata.get("clip_skip") is not None:
            compare_numeric_field("Clip skip", abs(int(expected_metadata["clip_skip"])))

        # Validate image dimensions
        if expected_metadata.get("image_width") and expected_metadata.get("image_height"):
            expected_size = f"{expected_metadata['image_width']}x{expected_metadata['image_height']}"
            actual_size = fields.get("Size")
            actual_size_str = normalize_value(actual_size)
            if actual_size in (None, ""):
                message = f"Size missing, expected '{expected_size}'"
                mark_fail("Size", expected_size, actual_size_str, message)
            elif actual_size_str != expected_size:
                message = (
                    f"Size mismatch: expected '{expected_size}', "
                    f"got '{actual_size_str}'"
                )
                mark_warn("Size", expected_size, actual_size_str, message)
            else:
                mark_pass("Size", expected_size, actual_size_str)

        # Validate LoRA stack
        expected_loras = expected_metadata.get("lora_stack")
        if expected_loras:
            lora_indices: set[int] = set()
            for key in fields.keys():
                match = re.match(r"Lora_(\d+) Model name", key)
                if match:
                    lora_indices.add(int(match.group(1)))

            actual_lora_count = len(lora_indices)
            expected_lora_count = len(expected_loras)
            if actual_lora_count == expected_lora_count:
                mark_pass("LoRA count", expected_lora_count, actual_lora_count)
            else:
                mark_fail(
                    "LoRA count",
                    expected_lora_count,
                    actual_lora_count,
                    f"LoRA count mismatch: expected {expected_lora_count} LoRAs, got {actual_lora_count}",
                )

            for idx, expected_lora in enumerate(expected_loras):
                name_key = f"Lora_{idx} Model name"
                model_str_key = f"Lora_{idx} Strength model"
                clip_str_key = f"Lora_{idx} Strength clip"

                if name_key in fields:
                    actual_name = fields[name_key]
                    actual_basename = Path(actual_name).stem if actual_name else ""
                    expected_basename = Path(expected_lora["name"]).stem
                    if actual_basename == expected_basename:
                        mark_pass(f"LoRA {idx} name", expected_basename, actual_basename)
                    else:
                        mark_fail(
                            f"LoRA {idx} name",
                            expected_basename,
                            actual_basename,
                            f"LoRA {idx} name mismatch: expected '{expected_basename}', got '{actual_basename}'",
                        )

                    if model_str_key in fields:
                        compare_numeric_field(model_str_key, expected_lora["model_strength"])
                    else:
                        mark_fail(
                            model_str_key,
                            expected_lora["model_strength"],
                            "N/A",
                            f"{model_str_key} not present in metadata",
                        )

                    if clip_str_key in fields:
                        compare_numeric_field(clip_str_key, expected_lora["clip_strength"])
                    else:
                        mark_fail(
                            clip_str_key,
                            expected_lora["clip_strength"],
                            "N/A",
                            f"{clip_str_key} not present in metadata",
                        )
                else:
                    mark_fail(
                        f"LoRA {idx} name",
                        Path(expected_lora["name"]).stem,
                        "N/A",
                        f"Expected LoRA {idx} ('{expected_lora['name']}') not found in metadata",
                    )

                hash_key = f"Lora_{idx} Model hash"
                if hash_key in fields:
                    hash_value = normalize_value(fields[hash_key])
                    if hash_value in ("", "N/A"):
                        mark_fail(
                            hash_key,
                            "computed hash",
                            hash_value,
                            f"{hash_key} missing hash value",
                        )
                    else:
                        mark_pass(hash_key, "computed hash", hash_value)
                else:
                    mark_fail(
                        hash_key,
                        "computed hash",
                        "N/A",
                        f"{hash_key} not present in metadata",
                    )

        # Prompts
        if expected_metadata.get("positive_prompt"):
            compare_string_field("Positive prompt", expected_metadata.get("positive_prompt"))

        if expected_metadata.get("negative_prompt"):
            compare_string_field("Negative prompt", expected_metadata.get("negative_prompt"))

        # Hash fields
        if expected_metadata.get("model_name"):
            actual_model_hash = fields.get("Model hash")
            actual_model_hash_str = normalize_value(actual_model_hash)
            if actual_model_hash in (None, ""):
                mark_fail("Model hash", "computed hash", actual_model_hash_str, "Model hash missing")
            elif actual_model_hash_str == "N/A":
                message = "Model hash is 'N/A' - should be computed"
                mark_fail("Model hash", "computed hash", actual_model_hash_str, message)
            else:
                mark_pass("Model hash", "computed hash", actual_model_hash_str)

        if expected_metadata.get("vae_name") and expected_metadata["vae_name"] != "Baked VAE":
            actual_vae_hash = fields.get("VAE hash")
            actual_vae_hash_str = normalize_value(actual_vae_hash)
            if actual_vae_hash in (None, ""):
                mark_fail("VAE hash", "computed hash", actual_vae_hash_str, "VAE hash missing")
            elif actual_vae_hash_str == "N/A":
                mark_fail("VAE hash", "computed hash", actual_vae_hash_str, "VAE hash is 'N/A' - should be computed")
            else:
                mark_pass("VAE hash", "computed hash", actual_vae_hash_str)

        if expected_metadata.get("embedding_hash"):
            actual_embedding_hash = fields.get("Embedding hash")
            actual_embedding_hash_str = normalize_value(actual_embedding_hash)
            if actual_embedding_hash in (None, ""):
                mark_fail("Embedding hash", "computed hash", actual_embedding_hash_str, "Embedding hash missing")
            elif actual_embedding_hash_str == "N/A":
                message = "Embedding hash is 'N/A' - should be computed"
                mark_fail("Embedding hash", "computed hash", actual_embedding_hash_str, message)
            else:
                mark_pass("Embedding hash", "computed hash", actual_embedding_hash_str)

        # Batch details
        batch_size_expected = expected_metadata.get("batch_size")
        if batch_size_expected not in (None, 1, "1"):
            compare_numeric_field("Batch size", batch_size_expected)

        if expected_metadata.get("batch_number") is not None:
            compare_numeric_field("Batch number", expected_metadata["batch_number"])

        # Additional Flux parameters
        if expected_metadata.get("base_shift") is not None:
            compare_numeric_field("Base shift", expected_metadata["base_shift"])

        if expected_metadata.get("max_shift") is not None:
            compare_numeric_field("Max shift", expected_metadata["max_shift"])

        if expected_metadata.get("shift") is not None:
            compare_numeric_field("Shift", expected_metadata["shift"])

        if expected_metadata.get("weight_dtype"):
            expected_dtype = str(expected_metadata["weight_dtype"])
            actual_dtype = fields.get("Weight dtype")
            actual_dtype_str = normalize_value(actual_dtype)
            if actual_dtype:
                if actual_dtype_str == expected_dtype:
                    mark_pass("Weight dtype", expected_dtype, actual_dtype_str)
                else:
                    mark_fail(
                        "Weight dtype",
                        expected_dtype,
                        actual_dtype_str,
                        f"Weight dtype mismatch: expected '{expected_dtype}', got '{actual_dtype_str}'",
                    )
            else:
                mark_fail("Weight dtype", expected_dtype, actual_dtype_str, "Weight dtype missing")

        if expected_metadata.get("clip_prompt"):
            compare_string_field("CLIP prompt", expected_metadata.get("clip_prompt"))

        if expected_metadata.get("t5_prompt"):
            compare_string_field("T5 prompt", expected_metadata.get("t5_prompt"))

        if expected_metadata.get("clip_model_name"):
            compare_string_field("CLIP model name", expected_metadata.get("clip_model_name"))

        if expected_metadata.get("embedding_name"):
            compare_string_field("Embedding name", expected_metadata.get("embedding_name"))

        # Store check count (exclude purely informational entries)
        result["checks_performed"] = sum(1 for detail in check_details if detail["status"] in {"pass", "fail", "warn"})

    def validate_image(
        self, image_path: Path, workflow_name: str, expected: dict, expected_save_node: dict | None = None
    ) -> dict:
        """Validate a single image's metadata.

        Args:
            image_path: Path to the image file
            workflow_name: Name of the workflow
            expected: Overall expected metadata from workflow
            expected_save_node: Specific expected metadata for this save node (if known)
        """
        result = {
            "image_path": str(image_path),
            "workflow_name": workflow_name,
            "passed": False,
            "errors": [],
            "warnings": [],
            "notes": [],
            "metadata_found": False,
            "fields": {},
            "check_details": [],
        }

        # Check if this is a control image (without metadata)
        # Control images are saved using the default SaveImage node and are expected
        # to have no metadata or parameters field
        is_control_image = "without-meta" in image_path.name.lower()

        # Read metadata
        metadata = MetadataReader.read_metadata(image_path)

        # Handle control images: they should have no metadata or parameters
        if is_control_image:
            if not metadata or not metadata.get("parameters", ""):
                # Expected behavior for control images
                result["passed"] = True
                result["warnings"].append("Control image (without-meta) - no metadata expected")
                result["checks_performed"] = 0
                return result
            # Control image unexpectedly has metadata - continue with normal validation
            # but add a warning
            result["warnings"].append("Control image (without-meta) has unexpected metadata - validating anyway")

        # For non-control images, metadata is required
        if not metadata:
            result["errors"].append("No metadata found in image")
            return result

        result["metadata_found"] = True

        # Get parameters string
        params_str = metadata.get("parameters", "")
        if not params_str:
            result["errors"].append("No 'parameters' field found in metadata")
            return result

        # Parse parameters
        fields = self.parse_parameters_string(params_str)
        result["fields"] = fields

        # Comprehensive validation if we have detailed expected metadata
        if expected_save_node:
            self._validate_expected_fields(fields, expected_save_node, result)
        else:
            # Fallback to basic validation
            # Check for required fields based on workflow
            required_fields = []
            if expected.get("save_nodes"):
                # Use first save node
                save_node = expected["save_nodes"][0]
                if save_node.get("steps"):
                    required_fields.append("Steps")
                if save_node.get("sampler_name"):
                    required_fields.append("Sampler")
                if save_node.get("cfg"):
                    required_fields.append("CFG scale")
                if save_node.get("seed") is not None:
                    required_fields.append("Seed")

            # Validate required fields
            check_details = result.setdefault("check_details", [])
            for field in required_fields:
                actual_value = fields.get(field)
                if actual_value is None:
                    result["errors"].append(f"Required field '{field}' not found in metadata")
                    check_details.append(
                        {
                            "field": field,
                            "status": "fail",
                            "expected": "present",
                            "actual": "N/A",
                        }
                    )
                else:
                    check_details.append(
                        {
                            "field": field,
                            "status": "pass",
                            "expected": "present",
                            "actual": actual_value,
                        }
                    )

            if required_fields and "checks_performed" not in result:
                result["checks_performed"] = sum(
                    1 for detail in check_details if detail.get("status") in {"pass", "fail", "warn"}
                )

        # Check for fallback indicator
        if "Metadata Fallback:" in params_str:
            fallback_match = re.search(r"Metadata Fallback:\s*(\S+)", params_str)
            if fallback_match:
                fallback_stage = fallback_match.group(1)
                result["warnings"].append(f"Metadata fallback occurred: {fallback_stage}")
                result["fallback_stage"] = fallback_stage

        # Check for N/A values in any field (should never happen)
        for field_name, field_value in fields.items():
            if field_value.strip() == "N/A":
                result["errors"].append(f"Field '{field_name}' contains 'N/A' value: {field_value}")

        # Validate Hashes summary if present
        if "Hashes" in fields:
            try:
                hashes_dict = json.loads(fields["Hashes"])
                self._validate_hashes_summary(fields, hashes_dict, result)
            except json.JSONDecodeError:
                result["errors"].append("Hashes field is not valid JSON")

        # Validate hashes against sidecar files (if models path provided)
        self._validate_hashes_against_sidecars(fields, self.comfyui_models_path, result)

        # Validate embedding fields
        self._validate_embedding_fields(fields, result)

        # Validate file format matches expectation
        def normalize_format(fmt: Any) -> str | None:
            if fmt is None or fmt == "":
                return None
            fmt_str = str(fmt).lower()
            return "jpeg" if fmt_str == "jpg" else fmt_str

        expected_format = normalize_format((expected_save_node or {}).get("file_format"))
        if expected_format is None:
            expected_format = normalize_format(expected.get("file_format", "png")) or "png"

        actual_format = normalize_format(image_path.suffix.lower().lstrip(".")) or ""

        if expected_format != actual_format:
            result["warnings"].append(f"File format mismatch: expected {expected_format}, got {actual_format}")

        # Mark as passed if no errors
        result["passed"] = len(result["errors"]) == 0

        return result

    def _validate_hashes_summary(self, fields: dict, hashes_dict: dict, result: dict):
        """Validate that the Hashes summary matches the metadata entries."""
        # Check that all models/VAEs/LoRAs/embeddings in metadata are in Hashes
        # LoRAs
        lora_indices = set()
        for key in fields.keys():
            if key.startswith("Lora_") and "Model name" in key:
                # Extract index
                match = re.match(r"Lora_(\d+) Model name", key)
                if match:
                    lora_indices.add(int(match.group(1)))

        for idx in lora_indices:
            model_name_key = f"Lora_{idx} Model name"
            model_hash_key = f"Lora_{idx} Model hash"

            if model_name_key in fields:
                model_name = fields[model_name_key]
                # Remove extension if present
                model_name_base = model_name.replace(".safetensors", "").replace(".pt", "").replace(".ckpt", "")

                # Check if this LoRA is in the Hashes dict
                lora_key = f"lora:{model_name_base}"
                if lora_key not in hashes_dict:
                    result["errors"].append(f"LoRA '{model_name}' has metadata but is missing from Hashes summary")
                else:
                    # Validate hash consistency
                    if model_hash_key in fields:
                        metadata_hash = fields[model_hash_key]
                        hashes_hash = hashes_dict[lora_key]
                        if metadata_hash != "N/A" and metadata_hash != hashes_hash:
                            result["errors"].append(
                                f"LoRA '{model_name}' hash mismatch: metadata has '{metadata_hash}' "
                                f"but Hashes summary has '{hashes_hash}'"
                            )

            if model_hash_key in fields and fields[model_hash_key] == "N/A":
                result["errors"].append(f"LoRA hash for Lora_{idx} is 'N/A' - hash should always be computed")

        # Embeddings
        embedding_indices = set()
        for key in fields.keys():
            if key.startswith("Embedding_") and "name" in key:
                match = re.match(r"Embedding_(\d+) name", key)
                if match:
                    embedding_indices.add(int(match.group(1)))

        for idx in embedding_indices:
            name_key = f"Embedding_{idx} name"
            hash_key = f"Embedding_{idx} hash"

            if name_key in fields:
                emb_name = fields[name_key]

                # Check if embedding is in Hashes dict
                # The key should be embed:<name>, not embed:<wrong_index>
                found_in_hashes = False
                hash_value_in_hashes = None

                for hash_key_name in hashes_dict.keys():
                    if hash_key_name.startswith("embed:"):
                        embed_name_in_hash = hash_key_name.replace("embed:", "")

                        # Check if the name matches exactly
                        if embed_name_in_hash == emb_name:
                            found_in_hashes = True
                            hash_value_in_hashes = hashes_dict[hash_key_name]
                            break
                        # If it's just a number, that's wrong - should be the embedding name
                        elif embed_name_in_hash.isdigit():
                            result["errors"].append(
                                f"Embedding_{idx} '{emb_name}' is in Hashes with wrong key "
                                f"'embed:{embed_name_in_hash}' (should be 'embed:{emb_name}')"
                            )
                            found_in_hashes = True  # Found but with wrong key
                            hash_value_in_hashes = hashes_dict[hash_key_name]
                            break

                if not found_in_hashes:
                    result["errors"].append(
                        f"Embedding_{idx} '{emb_name}' has metadata but is missing from Hashes summary"
                    )

                # Validate hash consistency between metadata and Hashes summary
                if found_in_hashes and hash_key in fields and hash_value_in_hashes:
                    metadata_hash = fields[hash_key]
                    if metadata_hash != "N/A" and metadata_hash != hash_value_in_hashes:
                        result["errors"].append(
                            f"Embedding_{idx} hash mismatch: metadata has '{metadata_hash}' "
                            f"but Hashes summary has '{hash_value_in_hashes}'"
                        )

            if hash_key in fields and fields[hash_key] == "N/A":
                result["errors"].append(f"Embedding hash for Embedding_{idx} is 'N/A' - hash should always be computed")

        # Check model and VAE
        if "Model hash" in fields:
            if fields["Model hash"] == "N/A":
                result["errors"].append("Model hash is 'N/A' - hash should always be computed")
            elif "model" in hashes_dict:
                metadata_hash = fields["Model hash"]
                hashes_hash = hashes_dict["model"]
                if metadata_hash != hashes_hash:
                    result["errors"].append(
                        f"Model hash mismatch: metadata has '{metadata_hash}' "
                        f"but Hashes summary has '{hashes_hash}'"
                    )

        if "VAE hash" in fields:
            if fields["VAE hash"] == "N/A":
                result["errors"].append("VAE hash is 'N/A' - hash should always be computed")
            elif "vae" in hashes_dict:
                metadata_hash = fields["VAE hash"]
                hashes_hash = hashes_dict["vae"]
                if metadata_hash != hashes_hash:
                    result["errors"].append(
                        f"VAE hash mismatch: metadata has '{metadata_hash}' " f"but Hashes summary has '{hashes_hash}'"
                    )

    def _validate_hash_against_sidecar(
        self, artifact_name: str, metadata_hash: str, artifact_type: str, comfyui_models_path: Path | None, result: dict
    ):
        """Validate hash from metadata against .sha256 sidecar file.

        Args:
            artifact_name: Name of the model/lora/vae/embedding file
            metadata_hash: Hash from the metadata (should be 10 chars)
            artifact_type: Type of artifact ("model", "lora", "vae", "embedding")
            comfyui_models_path: Path to ComfyUI models directory (optional)
            result: Result dict to append errors to
        """
        if not comfyui_models_path or not comfyui_models_path.exists():
            # Silently skip if models path not available
            return

        # Validate metadata hash is 10 characters
        if len(metadata_hash) != 10:
            result["errors"].append(
                f"{artifact_type.title()} '{artifact_name}' hash in metadata is not 10 characters: '{metadata_hash}'"
            )
            return

        # Try to find the artifact file
        # Common subdirectories for different artifact types
        # Based on user's ComfyUI setup:
        # unet/diffusion models: "diffusion_models", "DiffusionModels", "unet", "StableDiffusion"
        # embeddings: "Embeddings"
        # loras: "Lora"
        # ckpt: "StableDiffusion"
        # vae: "VAE"
        search_dirs = {
            "model": ["diffusion_models", "DiffusionModels", "unet", "StableDiffusion", "checkpoints"],
            "lora": ["Lora", "loras"],
            "vae": ["VAE", "vae"],
            "embedding": ["Embeddings", "embeddings"],
        }

        artifact_path = None
        for subdir in search_dirs.get(artifact_type, []):
            search_path = comfyui_models_path / subdir
            if search_path.exists():
                # Search recursively for the artifact
                for candidate in search_path.rglob(artifact_name):
                    if candidate.is_file():
                        artifact_path = candidate
                        break
                if artifact_path:
                    break

        if not artifact_path:
            # Artifact not found - log detailed search info for troubleshooting
            if self.verbose:
                searched_dirs = [str(comfyui_models_path / subdir) for subdir in search_dirs.get(artifact_type, [])]
                message = (
                    f"Hash validation: {artifact_type.title()} '{artifact_name}' not found. "
                    f"Searched in: {', '.join(searched_dirs)}"
                )
                result["warnings"].append(message)
            return

        # Check for sidecar file (prefer extension-less sidecar)
        sidecar_candidates = [
            artifact_path.with_suffix(".sha256"),
            artifact_path.with_suffix(artifact_path.suffix + ".sha256"),
        ]

        sidecar_path = None
        for candidate in sidecar_candidates:
            if candidate.exists():
                sidecar_path = candidate
                break

        if not sidecar_path:
            attempted = ", ".join(str(candidate) for candidate in sidecar_candidates)
            message = (
                f"Hash validation: {artifact_type.title()} '{artifact_name}' "
                f"has no .sha256 sidecar file (checked: {attempted})"
            )
            result["warnings"].append(message)
            return

        # Read sidecar file
        try:
            with open(sidecar_path, encoding="utf-8") as f:
                sidecar_hash = f.read().strip()

            # Validate sidecar hash is 64 characters
            if len(sidecar_hash) != 64:
                message = (
                    f"Hash validation FAILED: {artifact_type.title()} '{artifact_name}' "
                    f"sidecar hash is not 64 characters: '{sidecar_hash}'"
                )
                result["errors"].append(message)
                return

            # Validate sidecar hash is hex
            if not all(c in "0123456789abcdefABCDEF" for c in sidecar_hash):
                message = (
                    f"Hash validation FAILED: {artifact_type.title()} '{artifact_name}' "
                    f"sidecar hash is not valid hex: '{sidecar_hash}'"
                )
                result["errors"].append(message)
                return

            # Validate metadata hash matches first 10 characters of sidecar hash
            if metadata_hash.lower() != sidecar_hash[:10].lower():
                message = (
                    f"Hash validation FAILED: {artifact_type.title()} '{artifact_name}' hash mismatch: "
                    f"metadata has '{metadata_hash}' but sidecar first 10 chars are '{sidecar_hash[:10]}'"
                )
                result["errors"].append(message)
            else:
                # Hash validation passed - log in verbose mode
                if self.verbose:
                    message = (
                        f"Hash validation PASSED: {artifact_type.title()} '{artifact_name}' "
                        f"(metadata: {metadata_hash}, sidecar: {sidecar_hash[:10]}... [64 chars total])"
                    )
                    result.setdefault("notes", []).append(message)

        except Exception as e:
            message = f"Hash validation ERROR: Failed to read sidecar file for '{artifact_name}': {e}"
            result["warnings"].append(message)

    def _validate_hashes_against_sidecars(
        self, fields: dict, comfyui_models_path: Path | None, result: dict
    ):
        """Validate all hashes in metadata against their .sha256 sidecar files."""
        if not comfyui_models_path:
            return

        # Validate model hash
        if "Model" in fields and "Model hash" in fields:
            self._validate_hash_against_sidecar(
                fields["Model"], fields["Model hash"], "model", comfyui_models_path, result
            )

        # Validate VAE hash
        if "VAE" in fields and "VAE hash" in fields:
            self._validate_hash_against_sidecar(
                fields["VAE"], fields["VAE hash"], "vae", comfyui_models_path, result
            )

        # Validate LoRA hashes
        lora_indices = set()
        for key in fields.keys():
            if key.startswith("Lora_") and "Model name" in key:
                match = re.match(r"Lora_(\d+) Model name", key)
                if match:
                    lora_indices.add(int(match.group(1)))

        for idx in lora_indices:
            model_name_key = f"Lora_{idx} Model name"
            model_hash_key = f"Lora_{idx} Model hash"
            if model_name_key in fields and model_hash_key in fields:
                if fields[model_hash_key] != "N/A":
                    self._validate_hash_against_sidecar(
                        fields[model_name_key], fields[model_hash_key], "lora", comfyui_models_path, result
                    )

        # Validate embedding hashes
        embedding_indices = set()
        for key in fields.keys():
            if key.startswith("Embedding_") and "hash" in key:
                match = re.match(r"Embedding_(\d+) hash", key)
                if match:
                    embedding_indices.add(int(match.group(1)))

        for idx in embedding_indices:
            name_key = f"Embedding_{idx} name"
            hash_key = f"Embedding_{idx} hash"
            if name_key in fields and hash_key in fields:
                self._validate_hash_against_sidecar(
                    fields[name_key], fields[hash_key], "embedding", comfyui_models_path, result
                )

    def _validate_embedding_fields(self, fields: dict, result: dict):
        """Validate embedding-specific issues."""
        for key, value in fields.items():
            if "Embedding_" in key and "name" in key:
                # Check for trailing punctuation (commas, periods, semicolons, colons)
                if value.rstrip(",.;:") != value:
                    result["errors"].append(f"Embedding name '{key}' has trailing punctuation: '{value}'")

                # Check if this is actually a prompt (very long text suggests it's not an embedding)
                if len(value) > 100:
                    result["errors"].append(
                        f"Embedding name '{key}' appears to be a prompt (length={len(value)}), not an embedding name"
                    )

            # Check if embedding hash is also suspiciously long (suggests it's a prompt)
            # Normal hashes are typically 10-64 characters (sha256 truncated or full)
            if "Embedding_" in key and "hash" in key:
                if len(value) > 70:
                    result["errors"].append(
                        f"Embedding hash '{key}' appears to be a prompt (length={len(value)}), not a hash"
                    )

    def match_image_to_workflow(
        self, image_path: Path, filename_patterns: list[str], expected: dict[str, Any] | None = None
    ) -> bool:
        """Check if an image filename matches any of the workflow's filename patterns.

        Uses word-boundary matching to avoid false positives like "eff" matching "jeff_image.png".
        Patterns must match as whole words or be separated by delimiters (_, -, .).

        If no static patterns exist, falls back to checking if the image is in a subdirectory
        structure that matches the workflow's filename_prefix path pattern, or matches based
        on seed values if available.
        """
        if not filename_patterns:
            # If no patterns but we have expected metadata with a filename_prefix containing
            # path separators, check if the image is in a subdirectory structure
            if expected and expected.get("filename_prefix"):
                prefix = expected["filename_prefix"]
                # Check if prefix indicates subdirectory usage (contains / or \)
                if "/" in prefix or "\\" in prefix:
                    # Get relative path from output directory
                    try:
                        rel_path = image_path.relative_to(self.output_dir)
                        # If image is in a subdirectory (not directly in output_dir), consider it a match
                        if len(rel_path.parts) > 1:
                            return True
                    except ValueError:
                        # Image path is not relative to output_dir, continue with other matching strategies
                        pass

                # Try to match based on seed if available
                if expected.get("sampler_info"):
                    for sampler in expected["sampler_info"]:
                        seed = sampler.get("seed")
                        if seed is not None:
                            # Check if seed appears in filename
                            if str(seed) in image_path.stem:
                                return True
            return False

        # Remove file extension for matching
        image_name = image_path.stem.lower()

        # Check each pattern
        for pattern in filename_patterns:
            pattern_lower = pattern.lower()
            # Match pattern as a whole word or separated by delimiters (_,-,.)
            # Regex: (^|[_\-.])pattern($|[_\-.])
            regex = r"(^|[_\-.])" + re.escape(pattern_lower) + r"($|[_\-.])"
            if re.search(regex, image_name):
                return True

        return False

    def _print_validation_result(self, result: dict, save_node_metadata: dict | None = None):
        """Print validation result with optional verbose output."""
        status = "" if result["passed"] else ""
        checks = result.get("checks_performed", 0)
        image_path = Path(result["image_path"])
        print(f"    {status} {image_path.name} ({checks} checks)")

        # Print errors
        for error in result["errors"]:
            print(f"        Error: {error}")

        # Print warnings
        for warning in result["warnings"]:
            print(f"        Warning: {warning}")

        # Print informational notes (e.g., hash validation successes)
        if self.verbose:
            for note in result.get("notes", []):
                print(f"        Info: {note}")

        # In non-verbose mode, show key fields for passed validations
        if not self.verbose and result["passed"] and result.get("fields"):
            fields = result["fields"]
            if "Steps" in fields:
                print(f"        Steps: {fields['Steps']}")
            if "Sampler" in fields:
                print(f"        Sampler: {fields['Sampler']}")
            if "Seed" in fields:
                print(f"        Seed: {fields['Seed']}")

        # In verbose mode, show captured check details
        if self.verbose and result.get("metadata_found"):
            check_details = result.get("check_details", [])
            if check_details:
                print("        Validation Details:")
                for detail in check_details:
                    status = detail.get("status", "info")
                    symbol_map = {"pass": "", "fail": "", "warn": "", "info": ""}
                    symbol = symbol_map.get(status, "")
                    field_name = detail.get("field", "Field")
                    expected = detail.get("expected")
                    actual = detail.get("actual", "N/A")
                    if expected is not None and status != "info":
                        print(f"          {symbol} {field_name}: expected={expected}, actual={actual}")
                    else:
                        print(f"          {symbol} {field_name}: {actual}")

    def validate_workflow_outputs(self, workflow_file: Path, all_images: list[Path]) -> list[dict]:
        """Validate images generated by a specific workflow."""
        print(f"\nValidating workflow: {workflow_file.name}")

        # Special case: 1-scan-and-save-custom-metadata-rules.json doesn't create images
        # It only contains Metadata Rule Scanner + Save Custom Metadata Rules nodes
        if workflow_file.name == "1-scan-and-save-custom-metadata-rules.json":
            print("   Info: This workflow generates metadata rules, not images (skipping)")
            return []

        # Load workflow
        try:
            with open(workflow_file, encoding="utf-8") as f:
                workflow = json.load(f)
        except Exception as e:
            print(f"   Error loading workflow: {e}")
            return []

        # Extract expected metadata (now includes detailed save node info)
        expected = WorkflowAnalyzer.extract_expected_metadata(workflow, workflow_file.stem)

        if not expected["has_save_node"]:
            print("   Warning: No Save Image node found in workflow")
            return []

        # Show summary
        num_save_nodes = len(expected.get("save_nodes", []))
        print(f"  Found {num_save_nodes} Save Image node(s)")

        # Show the filename patterns we're looking for
        patterns = expected.get("filename_patterns", [])
        if patterns:
            print(f"  Filename patterns: {', '.join(patterns)}")

        # Filter images that match this workflow
        matching_images = []
        for image_path in all_images:
            if self.match_image_to_workflow(image_path, patterns, expected):
                matching_images.append(image_path)

        if not matching_images:
            print("   Warning: No matching images found for this workflow")
            return []

        print(f"  Found {len(matching_images)} matching image(s)")

        # Validate each matching image
        results = []
        for image_path in matching_images:
            # Try to match image to specific save node based on filename patterns
            best_save_node_match = None
            if expected.get("save_nodes"):
                image_name_lower = image_path.stem.lower()
                for save_node_metadata in expected["save_nodes"]:
                    # Use the filename_patterns extracted for this save node
                    patterns = save_node_metadata.get("filename_patterns", [])
                    if patterns:
                        for pattern in patterns:
                            pattern_lower = pattern.lower()
                            # Match pattern as whole word or separated by delimiters
                            regex = r"(^|[_\-.])" + re.escape(pattern_lower) + r"($|[_\-.])"
                            if re.search(regex, image_name_lower):
                                best_save_node_match = save_node_metadata
                                break
                        if best_save_node_match:
                            break

                # If no match found, use first save node
                if not best_save_node_match and expected["save_nodes"]:
                    best_save_node_match = expected["save_nodes"][0]

            # Validate with the matched save node's expected metadata
            result = self.validate_image(image_path, workflow_file.stem, expected, best_save_node_match)
            results.append(result)

            # Print result (with verbose option support)
            self._print_validation_result(result, best_save_node_match)

        return results

    def run_validation(self, extra_workflows_dir: Path | None = None) -> tuple[int, int, int]:
        """Run validation on all workflows and return (total, passed, failed).

        Args:
            extra_workflows_dir: Optional additional directory containing workflow JSON files
        """
        print("=" * 70)
        print("ComfyUI Metadata Validation")
        print("=" * 70)
        print(f"Workflow Dir: {self.workflow_dir}")
        if extra_workflows_dir:
            print(f"Extra Workflows: {extra_workflows_dir}")
        print(f"Output Dir:   {self.output_dir}")
        print("=" * 70)

        if not self.workflow_dir.exists():
            print(f" Error: Workflow directory not found: {self.workflow_dir}")
            return 0, 0, 0

        if not self.output_dir.exists():
            print(f" Error: Output directory not found: {self.output_dir}")
            return 0, 0, 0

        if not self.comfyui_models_path:
            print(" Hash validation disabled: pass --models-path to enable sidecar checks")

        # Find all workflow files from both directories
        workflow_files = sorted(self.workflow_dir.glob("*.json"))

        if extra_workflows_dir:
            if extra_workflows_dir.exists():
                extra_workflow_files = sorted(extra_workflows_dir.glob("*.json"))
                workflow_files.extend(extra_workflow_files)
                print(f"Added {len(extra_workflow_files)} workflow(s) from extra directory")
            else:
                print(f" Warning: Extra workflows directory not found: {extra_workflows_dir}")

        if not workflow_files:
            print(f" No workflow files found in {self.workflow_dir}")
            return 0, 0, 0

        # Collect all images once (more efficient than searching for each workflow)
        print("\nScanning for images...")
        image_suffixes = {".png", ".jpg", ".jpeg", ".webp"}
        all_images = [f for f in self.output_dir.rglob("*") if f.is_file() and f.suffix.lower() in image_suffixes]
        print(f"Found {len(all_images)} total image(s) in output directory")

        if not all_images:
            print(f" Warning: No images found in {self.output_dir}")
            return 0, 0, 0

        # Validate each workflow's outputs
        all_results = []
        validated_images = set()
        workflows_with_images = set()
        workflows_without_images = set()

        for workflow_file in workflow_files:
            results = self.validate_workflow_outputs(workflow_file, all_images)

            # Track workflows that had matching images vs those that didn't
            if results:
                workflows_with_images.add(workflow_file.name)
                all_results.extend(results)

                # Track which images were validated
                for result in results:
                    validated_images.add(Path(result["image_path"]))
            else:
                # Check if this workflow was skipped (like 1-scan-and-save-custom-metadata-rules.json)
                # or if it genuinely had no matching images
                if workflow_file.name != "1-scan-and-save-custom-metadata-rules.json":
                    # Load workflow to check if it has a save node
                    try:
                        with open(workflow_file, encoding="utf-8") as f:
                            workflow = json.load(f)
                        expected = WorkflowAnalyzer.extract_expected_metadata(workflow, workflow_file.stem)
                        if expected["has_save_node"]:
                            workflows_without_images.add(workflow_file.name)
                    except Exception as e:
                        print(f"Warning: Failed to analyze workflow '{workflow_file.name}': {e}")

        # Calculate statistics
        total = len(all_results)
        passed = sum(1 for r in all_results if r["passed"])
        failed = total - passed
        unmatched_images = set(all_images) - validated_images

        # Print summary
        print("\n" + "=" * 70)
        print("Validation Summary:")
        print(f"  Total Images Validated: {total}")
        print(f"   Passed:               {passed}")
        print(f"   Failed:               {failed}")
        print(f"   Unmatched Images:     {len(unmatched_images)}")
        print(f"   Unmatched Workflows:  {len(workflows_without_images)}")
        print("=" * 70)

        # Report unmatched images
        if unmatched_images:
            print(f"\nUnmatched Images ({len(unmatched_images)}):")
            for img in sorted(unmatched_images):
                print(f"  - {img.name}")

        # Report unmatched workflows
        if workflows_without_images:
            print(f"\nUnmatched Workflows ({len(workflows_without_images)}):")
            for wf in sorted(workflows_without_images):
                print(f"  - {wf}")

        if failed > 0:
            print("\nFailed Images:")
            for result in all_results:
                if not result["passed"]:
                    print(f"  - {Path(result['image_path']).name} (workflow: {result['workflow_name']})")
                    for error in result["errors"]:
                        print(f"      {error}")

        print("=" * 70)

        self.results = all_results
        return total, passed, failed


def main():
    parser = argparse.ArgumentParser(
        description="Validate metadata in ComfyUI workflow test outputs",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Windows
  python validate_metadata.py ^
    --output-folder "C:\\StableDiffusion\\StabilityMatrix-win-x64\\Data\\Packages\\ComfyUI_windows_portable\\ComfyUI\\output\\Test"

  # Linux/Mac
  python validate_metadata.py --output-folder "/path/to/ComfyUI/output/Test"

  # Custom workflow directory
  python validate_metadata.py --output-folder "./output/Test" --workflow-dir "./my_workflows"
        """,
    )

    parser.add_argument(
        "--output-folder",
        type=str,
        required=True,
        help="Path to ComfyUI output Test folder containing generated images",
    )

    parser.add_argument(
        "--workflow-dir",
        type=str,
        default="dev_test_workflows",
        help=(
            "Directory containing workflow JSON files (default resolves to "
            "tests/comfyui_cli_tests/dev_test_workflows)"
        ),
    )

    parser.add_argument(
        "--log-file",
        type=str,
        default=None,
        help="Path to write a copy of all console output (txt). Default: <output-folder>/validation_log.txt",
    )

    parser.add_argument(
        "--models-path",
        type=str,
        default=None,
        help="Path to ComfyUI models directory for validating hashes against .sha256 sidecar files (optional)",
    )

    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Show detailed validation results for every check performed"
    )

    parser.add_argument(
        "--extra-workflows",
        type=str,
        default=None,
        help="Additional directory containing workflow JSON files to validate (optional)",
    )

    args = parser.parse_args()

    # Convert to absolute paths
    workflow_dir = _resolve_relative_path(args.workflow_dir, fallback=CLI_COMPAT_DIR)
    if workflow_dir is None:
        print(" Error: Unable to resolve workflow directory path.")
        return 1
    output_dir = Path(args.output_folder)
    models_path = Path(args.models_path) if args.models_path else None
    extra_workflows_dir = _resolve_relative_path(args.extra_workflows, fallback=CLI_COMPAT_DIR)

    # Setup logging
    # Determine log file path
    log_path = Path(args.log_file) if args.log_file else (output_dir / "validation_log.txt")
    setup_print_tee(log_path)

    # Create validator and run
    validator = MetadataValidator(workflow_dir, output_dir, models_path)
    validator.verbose = args.verbose  # Pass verbose flag to validator
    total, passed, failed = validator.run_validation(extra_workflows_dir)

    # Exit with appropriate code
    return 0 if failed == 0 and total > 0 else 1


if __name__ == "__main__":
    sys.exit(main())
